{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done loading labels\n",
      "Loading Image:  0\n",
      "Loading Image:  100\n",
      "Loading Image:  200\n",
      "Loading Image:  300\n",
      "Loading Image:  400\n",
      "Loading Image:  500\n",
      "Loading Image:  600\n",
      "Loading Image:  700\n",
      "Loading Image:  800\n",
      "Loading Image:  900\n",
      "Loading Image:  1000\n",
      "Loading Image:  1100\n",
      "Loading Image:  1200\n",
      "Loading Image:  1300\n",
      "Loading Image:  1400\n",
      "Loading Image:  1500\n",
      "Loading Image:  1600\n",
      "Loading Image:  1700\n",
      "Loading Image:  1800\n",
      "Loading Image:  1900\n",
      "Loading Image:  2000\n",
      "Loading Image:  2100\n",
      "Loading Image:  2200\n",
      "Loading Image:  2300\n",
      "Loading Image:  2400\n",
      "Loading Image:  2500\n",
      "Loading Image:  2600\n",
      "Loading Image:  2700\n",
      "Loading Image:  2800\n",
      "Loading Image:  2900\n",
      "Loading Image:  3000\n",
      "Loading Image:  3100\n",
      "Loading Image:  3200\n",
      "Loading Image:  3300\n",
      "Loading Image:  3400\n",
      "Loading Image:  3500\n",
      "Loading Image:  3600\n",
      "Loading Image:  3700\n",
      "Loading Image:  3800\n",
      "Loading Image:  3900\n",
      "Loading Image:  4000\n",
      "Loading Image:  4100\n",
      "Loading Image:  4200\n",
      "Loading Image:  4300\n",
      "Loading Image:  4400\n",
      "Loading Image:  4500\n",
      "Loading Image:  4600\n",
      "Loading Image:  4700\n",
      "Loading Image:  4800\n",
      "Loading Image:  4900\n",
      "done loading images\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done loading labels\n",
      "Loading Image:  0\n",
      "Loading Image:  100\n",
      "Loading Image:  200\n",
      "Loading Image:  300\n",
      "Loading Image:  400\n",
      "Loading Image:  500\n",
      "Loading Image:  600\n",
      "Loading Image:  700\n",
      "Loading Image:  800\n",
      "Loading Image:  900\n",
      "Loading Image:  1000\n",
      "Loading Image:  1100\n",
      "Loading Image:  1200\n",
      "Loading Image:  1300\n",
      "Loading Image:  1400\n",
      "Loading Image:  1500\n",
      "Loading Image:  1600\n",
      "Loading Image:  1700\n",
      "Loading Image:  1800\n",
      "Loading Image:  1900\n",
      "Loading Image:  2000\n",
      "Loading Image:  2100\n",
      "Loading Image:  2200\n",
      "Loading Image:  2300\n",
      "Loading Image:  2400\n",
      "Loading Image:  2500\n",
      "Loading Image:  2600\n",
      "Loading Image:  2700\n",
      "Loading Image:  2800\n",
      "Loading Image:  2900\n",
      "Loading Image:  3000\n",
      "Loading Image:  3100\n",
      "Loading Image:  3200\n",
      "Loading Image:  3300\n",
      "Loading Image:  3400\n",
      "Loading Image:  3500\n",
      "Loading Image:  3600\n",
      "Loading Image:  3700\n",
      "Loading Image:  3800\n",
      "Loading Image:  3900\n",
      "Loading Image:  4000\n",
      "Loading Image:  4100\n",
      "Loading Image:  4200\n",
      "Loading Image:  4300\n",
      "Loading Image:  4400\n",
      "Loading Image:  4500\n",
      "Loading Image:  4600\n",
      "Loading Image:  4700\n",
      "Loading Image:  4800\n",
      "Loading Image:  4900\n",
      "done loading images\n",
      "Epoch [1/10], Iter [1/45] Loss: 6.5628\n",
      "Epoch [1/10], Iter [2/45] Loss: 7.0214\n",
      "Epoch [1/10], Iter [3/45] Loss: 6.2729\n",
      "Epoch [1/10], Iter [4/45] Loss: 6.6842\n",
      "Epoch [1/10], Iter [5/45] Loss: 6.1100\n",
      "Epoch [1/10], Iter [6/45] Loss: 6.9562\n",
      "Epoch [1/10], Iter [7/45] Loss: 6.5392\n",
      "Epoch [1/10], Iter [8/45] Loss: 6.1648\n",
      "Epoch [1/10], Iter [9/45] Loss: 6.7222\n",
      "Epoch [1/10], Iter [10/45] Loss: 6.2979\n",
      "Epoch [1/10], Iter [11/45] Loss: 6.4394\n",
      "Epoch [1/10], Iter [12/45] Loss: 6.1700\n",
      "Epoch [1/10], Iter [13/45] Loss: 6.4460\n",
      "Epoch [1/10], Iter [14/45] Loss: 6.6176\n",
      "Epoch [1/10], Iter [15/45] Loss: 6.5988\n",
      "Epoch [1/10], Iter [16/45] Loss: 6.0957\n",
      "Epoch [1/10], Iter [17/45] Loss: 5.9302\n",
      "Epoch [1/10], Iter [18/45] Loss: 6.6150\n",
      "Epoch [1/10], Iter [19/45] Loss: 5.8798\n",
      "Epoch [1/10], Iter [20/45] Loss: 5.8456\n",
      "Epoch [1/10], Iter [21/45] Loss: 5.9051\n",
      "Epoch [1/10], Iter [22/45] Loss: 5.7164\n",
      "Epoch [1/10], Iter [23/45] Loss: 5.5520\n",
      "Epoch [1/10], Iter [24/45] Loss: 5.8236\n",
      "Epoch [1/10], Iter [25/45] Loss: 6.0423\n",
      "Epoch [1/10], Iter [26/45] Loss: 6.3499\n",
      "Epoch [1/10], Iter [27/45] Loss: 6.5590\n",
      "Epoch [1/10], Iter [28/45] Loss: 5.4996\n",
      "Epoch [1/10], Iter [29/45] Loss: 6.4904\n",
      "Epoch [1/10], Iter [30/45] Loss: 5.8354\n",
      "Epoch [1/10], Iter [31/45] Loss: 5.6392\n",
      "Epoch [1/10], Iter [32/45] Loss: 6.2235\n",
      "Epoch [1/10], Iter [33/45] Loss: 5.2412\n",
      "Epoch [1/10], Iter [34/45] Loss: 6.2758\n",
      "Epoch [1/10], Iter [35/45] Loss: 5.7770\n",
      "Epoch [1/10], Iter [36/45] Loss: 6.5726\n",
      "Epoch [1/10], Iter [37/45] Loss: 6.3568\n",
      "Epoch [1/10], Iter [38/45] Loss: 5.9685\n",
      "Epoch [1/10], Iter [39/45] Loss: 5.1951\n",
      "Epoch [1/10], Iter [40/45] Loss: 5.3050\n",
      "Epoch [1/10], Iter [41/45] Loss: 6.4601\n",
      "Epoch [1/10], Iter [42/45] Loss: 6.5943\n",
      "Epoch [1/10], Iter [43/45] Loss: 5.6177\n",
      "Epoch [1/10], Iter [44/45] Loss: 5.7738\n",
      "Epoch [1/10], Iter [45/45] Loss: 5.6386\n",
      "Epoch [1/10], Iter [46/45] Loss: 5.8761\n",
      "Epoch [1/10], Iter [47/45] Loss: 6.1222\n",
      "Epoch [1/10], Iter [48/45] Loss: 5.2429\n",
      "Epoch [1/10], Iter [49/45] Loss: 5.3918\n",
      "Epoch [1/10], Iter [50/45] Loss: 5.5780\n",
      "Epoch [1/10], Iter [51/45] Loss: 5.7668\n",
      "Epoch [1/10], Iter [52/45] Loss: 5.3752\n",
      "Epoch [1/10], Iter [53/45] Loss: 5.5961\n",
      "Epoch [1/10], Iter [54/45] Loss: 5.7724\n",
      "Epoch [1/10], Iter [55/45] Loss: 6.1538\n",
      "Epoch [1/10], Iter [56/45] Loss: 5.3420\n",
      "Epoch [1/10], Iter [57/45] Loss: 5.6062\n",
      "Epoch [1/10], Iter [58/45] Loss: 5.2771\n",
      "Epoch [1/10], Iter [59/45] Loss: 5.6756\n",
      "Epoch [1/10], Iter [60/45] Loss: 6.3423\n",
      "Epoch [1/10], Iter [61/45] Loss: 5.8291\n",
      "Epoch [1/10], Iter [62/45] Loss: 5.1304\n",
      "Epoch [1/10], Iter [63/45] Loss: 5.6676\n",
      "Epoch [1/10], Iter [64/45] Loss: 5.6864\n",
      "Epoch [1/10], Iter [65/45] Loss: 5.3089\n",
      "Epoch [1/10], Iter [66/45] Loss: 5.4421\n",
      "Epoch [1/10], Iter [67/45] Loss: 5.2435\n",
      "Epoch [1/10], Iter [68/45] Loss: 6.5226\n",
      "Epoch [1/10], Iter [69/45] Loss: 5.3770\n",
      "Epoch [1/10], Iter [70/45] Loss: 5.7870\n",
      "Epoch [1/10], Iter [71/45] Loss: 5.1195\n",
      "Epoch [1/10], Iter [72/45] Loss: 6.0836\n",
      "Epoch [1/10], Iter [73/45] Loss: 5.5473\n",
      "Epoch [1/10], Iter [74/45] Loss: 5.7143\n",
      "Epoch [1/10], Iter [75/45] Loss: 5.4299\n",
      "Epoch [1/10], Iter [76/45] Loss: 4.9331\n",
      "Epoch [1/10], Iter [77/45] Loss: 4.9582\n",
      "Epoch [1/10], Iter [78/45] Loss: 4.8226\n",
      "Epoch [1/10], Iter [79/45] Loss: 5.5869\n",
      "Epoch [1/10], Iter [80/45] Loss: 5.7344\n",
      "Epoch [1/10], Iter [81/45] Loss: 5.2516\n",
      "Epoch [1/10], Iter [82/45] Loss: 5.5246\n",
      "Epoch [1/10], Iter [83/45] Loss: 5.2909\n",
      "Epoch [1/10], Iter [84/45] Loss: 5.7828\n",
      "Epoch [1/10], Iter [85/45] Loss: 5.6126\n",
      "Epoch [1/10], Iter [86/45] Loss: 4.8490\n",
      "Epoch [1/10], Iter [87/45] Loss: 5.3251\n",
      "Epoch [1/10], Iter [88/45] Loss: 5.6550\n",
      "Epoch [1/10], Iter [89/45] Loss: 5.2920\n",
      "Epoch [1/10], Iter [90/45] Loss: 4.8166\n",
      "Epoch [1/10], Iter [91/45] Loss: 5.0523\n",
      "Epoch [1/10], Iter [92/45] Loss: 6.0124\n",
      "Epoch [1/10], Iter [93/45] Loss: 5.7867\n",
      "Epoch [1/10], Iter [94/45] Loss: 4.5315\n",
      "Epoch [1/10], Iter [95/45] Loss: 5.2009\n",
      "Epoch [1/10], Iter [96/45] Loss: 4.6160\n",
      "Epoch [1/10], Iter [97/45] Loss: 5.0230\n",
      "Epoch [1/10], Iter [98/45] Loss: 5.0162\n",
      "Epoch [1/10], Iter [99/45] Loss: 5.0009\n",
      "Epoch [1/10], Iter [100/45] Loss: 4.6624\n",
      "Epoch [1/10], Iter [101/45] Loss: 4.1835\n",
      "Epoch [1/10], Iter [102/45] Loss: 5.5151\n",
      "Epoch [1/10], Iter [103/45] Loss: 4.5519\n",
      "Epoch [1/10], Iter [104/45] Loss: 4.7103\n",
      "Epoch [1/10], Iter [105/45] Loss: 4.6453\n",
      "Epoch [1/10], Iter [106/45] Loss: 4.9480\n",
      "Epoch [1/10], Iter [107/45] Loss: 5.2029\n",
      "Epoch [1/10], Iter [108/45] Loss: 4.8306\n",
      "Epoch [1/10], Iter [109/45] Loss: 5.0625\n",
      "Epoch [1/10], Iter [110/45] Loss: 4.7603\n",
      "Epoch [1/10], Iter [111/45] Loss: 4.7454\n",
      "Epoch [1/10], Iter [112/45] Loss: 4.6935\n",
      "Epoch [1/10], Iter [113/45] Loss: 5.1337\n",
      "Epoch [1/10], Iter [114/45] Loss: 4.8659\n",
      "Epoch [1/10], Iter [115/45] Loss: 5.1917\n",
      "Epoch [1/10], Iter [116/45] Loss: 4.6526\n",
      "Epoch [1/10], Iter [117/45] Loss: 4.7119\n",
      "Epoch [1/10], Iter [118/45] Loss: 4.6716\n",
      "Epoch [1/10], Iter [119/45] Loss: 5.1982\n",
      "Epoch [1/10], Iter [120/45] Loss: 4.6392\n",
      "Epoch [1/10], Iter [121/45] Loss: 4.5015\n",
      "Epoch [1/10], Iter [122/45] Loss: 4.4455\n",
      "Epoch [1/10], Iter [123/45] Loss: 4.1813\n",
      "Epoch [1/10], Iter [124/45] Loss: 4.5442\n",
      "Epoch [1/10], Iter [125/45] Loss: 4.5786\n",
      "Epoch [1/10], Iter [126/45] Loss: 4.7456\n",
      "Epoch [1/10], Iter [127/45] Loss: 4.3067\n",
      "Epoch [1/10], Iter [128/45] Loss: 4.3890\n",
      "Epoch [1/10], Iter [129/45] Loss: 4.0955\n",
      "Epoch [1/10], Iter [130/45] Loss: 4.6464\n",
      "Epoch [1/10], Iter [131/45] Loss: 5.1757\n",
      "Epoch [1/10], Iter [132/45] Loss: 4.1615\n",
      "Epoch [1/10], Iter [133/45] Loss: 5.0924\n",
      "Epoch [1/10], Iter [134/45] Loss: 4.2692\n",
      "Epoch [1/10], Iter [135/45] Loss: 4.4202\n",
      "Epoch [1/10], Iter [136/45] Loss: 4.3049\n",
      "Epoch [1/10], Iter [137/45] Loss: 4.5254\n",
      "Epoch [1/10], Iter [138/45] Loss: 4.4705\n",
      "Epoch [1/10], Iter [139/45] Loss: 4.9120\n",
      "Epoch [1/10], Iter [140/45] Loss: 5.1177\n",
      "Epoch [1/10], Iter [141/45] Loss: 4.6080\n",
      "Epoch [1/10], Iter [142/45] Loss: 4.8625\n",
      "Epoch [1/10], Iter [143/45] Loss: 4.4470\n",
      "Epoch [1/10], Iter [144/45] Loss: 4.5578\n",
      "Epoch [1/10], Iter [145/45] Loss: 4.1508\n",
      "Epoch [1/10], Iter [146/45] Loss: 4.2108\n",
      "Epoch [1/10], Iter [147/45] Loss: 4.9602\n",
      "Epoch [1/10], Iter [148/45] Loss: 5.0117\n",
      "Epoch [1/10], Iter [149/45] Loss: 4.5447\n",
      "Epoch [1/10], Iter [150/45] Loss: 4.5586\n",
      "Epoch [1/10], Iter [151/45] Loss: 4.5015\n",
      "Epoch [1/10], Iter [152/45] Loss: 5.0624\n",
      "Epoch [1/10], Iter [153/45] Loss: 3.9527\n",
      "Epoch [1/10], Iter [154/45] Loss: 4.6718\n",
      "Epoch [1/10], Iter [155/45] Loss: 4.6349\n",
      "Epoch [1/10], Iter [156/45] Loss: 4.1731\n",
      "Epoch [1/10], Iter [157/45] Loss: 4.3508\n",
      "Epoch [1/10], Iter [158/45] Loss: 4.3549\n",
      "Epoch [1/10], Iter [159/45] Loss: 3.4510\n",
      "Epoch [1/10], Iter [160/45] Loss: 4.5015\n",
      "Epoch [1/10], Iter [161/45] Loss: 4.1239\n",
      "Epoch [1/10], Iter [162/45] Loss: 3.6647\n",
      "Epoch [1/10], Iter [163/45] Loss: 4.3253\n",
      "Epoch [1/10], Iter [164/45] Loss: 4.4491\n",
      "Epoch [1/10], Iter [165/45] Loss: 4.0650\n",
      "Epoch [1/10], Iter [166/45] Loss: 4.1963\n",
      "Epoch [1/10], Iter [167/45] Loss: 4.4949\n",
      "Epoch [1/10], Iter [168/45] Loss: 4.1554\n",
      "Epoch [1/10], Iter [169/45] Loss: 4.3317\n",
      "Epoch [1/10], Iter [170/45] Loss: 3.7628\n",
      "Epoch [1/10], Iter [171/45] Loss: 4.0819\n",
      "Epoch [1/10], Iter [172/45] Loss: 3.8852\n",
      "Epoch [1/10], Iter [173/45] Loss: 4.1468\n",
      "Epoch [1/10], Iter [174/45] Loss: 4.1197\n",
      "Epoch [1/10], Iter [175/45] Loss: 4.0540\n",
      "Epoch [1/10], Iter [176/45] Loss: 4.1615\n",
      "Epoch [1/10], Iter [177/45] Loss: 4.6927\n",
      "Epoch [1/10], Iter [178/45] Loss: 4.0850\n",
      "Epoch [1/10], Iter [179/45] Loss: 3.9755\n",
      "Epoch [1/10], Iter [180/45] Loss: 3.9545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "Epoch [2/10], Iter [1/45] Loss: 3.9267\n",
      "Epoch [2/10], Iter [2/45] Loss: 3.8442\n",
      "Epoch [2/10], Iter [3/45] Loss: 3.3989\n",
      "Epoch [2/10], Iter [4/45] Loss: 4.0542\n",
      "Epoch [2/10], Iter [5/45] Loss: 4.1140\n",
      "Epoch [2/10], Iter [6/45] Loss: 4.0529\n",
      "Epoch [2/10], Iter [7/45] Loss: 3.9649\n",
      "Epoch [2/10], Iter [8/45] Loss: 4.2267\n",
      "Epoch [2/10], Iter [9/45] Loss: 3.9028\n",
      "Epoch [2/10], Iter [10/45] Loss: 4.9105\n",
      "Epoch [2/10], Iter [11/45] Loss: 4.5295\n",
      "Epoch [2/10], Iter [12/45] Loss: 3.3674\n",
      "Epoch [2/10], Iter [13/45] Loss: 4.1949\n",
      "Epoch [2/10], Iter [14/45] Loss: 4.2869\n",
      "Epoch [2/10], Iter [15/45] Loss: 4.0457\n",
      "Epoch [2/10], Iter [16/45] Loss: 3.7500\n",
      "Epoch [2/10], Iter [17/45] Loss: 3.9449\n",
      "Epoch [2/10], Iter [18/45] Loss: 4.2983\n",
      "Epoch [2/10], Iter [19/45] Loss: 3.2234\n",
      "Epoch [2/10], Iter [20/45] Loss: 3.5613\n",
      "Epoch [2/10], Iter [21/45] Loss: 3.5002\n",
      "Epoch [2/10], Iter [22/45] Loss: 3.6267\n",
      "Epoch [2/10], Iter [23/45] Loss: 3.8773\n",
      "Epoch [2/10], Iter [24/45] Loss: 3.4340\n",
      "Epoch [2/10], Iter [25/45] Loss: 3.5688\n",
      "Epoch [2/10], Iter [26/45] Loss: 4.1519\n",
      "Epoch [2/10], Iter [27/45] Loss: 3.4122\n",
      "Epoch [2/10], Iter [28/45] Loss: 3.4475\n",
      "Epoch [2/10], Iter [29/45] Loss: 3.2422\n",
      "Epoch [2/10], Iter [30/45] Loss: 3.7543\n",
      "Epoch [2/10], Iter [31/45] Loss: 3.4147\n",
      "Epoch [2/10], Iter [32/45] Loss: 3.4833\n",
      "Epoch [2/10], Iter [33/45] Loss: 3.7917\n",
      "Epoch [2/10], Iter [34/45] Loss: 3.2661\n",
      "Epoch [2/10], Iter [35/45] Loss: 3.3477\n",
      "Epoch [2/10], Iter [36/45] Loss: 3.3836\n",
      "Epoch [2/10], Iter [37/45] Loss: 4.0750\n",
      "Epoch [2/10], Iter [38/45] Loss: 3.6912\n",
      "Epoch [2/10], Iter [39/45] Loss: 3.6618\n",
      "Epoch [2/10], Iter [40/45] Loss: 3.8429\n",
      "Epoch [2/10], Iter [41/45] Loss: 3.7318\n",
      "Epoch [2/10], Iter [42/45] Loss: 3.3689\n",
      "Epoch [2/10], Iter [43/45] Loss: 3.4579\n",
      "Epoch [2/10], Iter [44/45] Loss: 3.9926\n",
      "Epoch [2/10], Iter [45/45] Loss: 3.2947\n",
      "Epoch [2/10], Iter [46/45] Loss: 3.6684\n",
      "Epoch [2/10], Iter [47/45] Loss: 3.6876\n",
      "Epoch [2/10], Iter [48/45] Loss: 4.3749\n",
      "Epoch [2/10], Iter [49/45] Loss: 3.0345\n",
      "Epoch [2/10], Iter [50/45] Loss: 3.8281\n",
      "Epoch [2/10], Iter [51/45] Loss: 3.1926\n",
      "Epoch [2/10], Iter [52/45] Loss: 3.6406\n",
      "Epoch [2/10], Iter [53/45] Loss: 3.3412\n",
      "Epoch [2/10], Iter [54/45] Loss: 3.5736\n",
      "Epoch [2/10], Iter [55/45] Loss: 3.2356\n",
      "Epoch [2/10], Iter [56/45] Loss: 3.4223\n",
      "Epoch [2/10], Iter [57/45] Loss: 3.3266\n",
      "Epoch [2/10], Iter [58/45] Loss: 3.9576\n",
      "Epoch [2/10], Iter [59/45] Loss: 3.7620\n",
      "Epoch [2/10], Iter [60/45] Loss: 3.4549\n",
      "Epoch [2/10], Iter [61/45] Loss: 3.6849\n",
      "Epoch [2/10], Iter [62/45] Loss: 3.3423\n",
      "Epoch [2/10], Iter [63/45] Loss: 3.2820\n",
      "Epoch [2/10], Iter [64/45] Loss: 3.9552\n",
      "Epoch [2/10], Iter [65/45] Loss: 4.4079\n",
      "Epoch [2/10], Iter [66/45] Loss: 3.0117\n",
      "Epoch [2/10], Iter [67/45] Loss: 3.5594\n",
      "Epoch [2/10], Iter [68/45] Loss: 3.1164\n",
      "Epoch [2/10], Iter [69/45] Loss: 3.5410\n",
      "Epoch [2/10], Iter [70/45] Loss: 3.1630\n",
      "Epoch [2/10], Iter [71/45] Loss: 3.4384\n",
      "Epoch [2/10], Iter [72/45] Loss: 3.9169\n",
      "Epoch [2/10], Iter [73/45] Loss: 3.3852\n",
      "Epoch [2/10], Iter [74/45] Loss: 3.3558\n",
      "Epoch [2/10], Iter [75/45] Loss: 3.4838\n",
      "Epoch [2/10], Iter [76/45] Loss: 3.3675\n",
      "Epoch [2/10], Iter [77/45] Loss: 3.5260\n",
      "Epoch [2/10], Iter [78/45] Loss: 2.6753\n",
      "Epoch [2/10], Iter [79/45] Loss: 3.5891\n",
      "Epoch [2/10], Iter [80/45] Loss: 3.6158\n",
      "Epoch [2/10], Iter [81/45] Loss: 3.2894\n",
      "Epoch [2/10], Iter [82/45] Loss: 2.9699\n",
      "Epoch [2/10], Iter [83/45] Loss: 3.2868\n",
      "Epoch [2/10], Iter [84/45] Loss: 3.2607\n",
      "Epoch [2/10], Iter [85/45] Loss: 2.6899\n",
      "Epoch [2/10], Iter [86/45] Loss: 3.1779\n",
      "Epoch [2/10], Iter [87/45] Loss: 2.4908\n",
      "Epoch [2/10], Iter [88/45] Loss: 3.0605\n",
      "Epoch [2/10], Iter [89/45] Loss: 2.8468\n",
      "Epoch [2/10], Iter [90/45] Loss: 3.2441\n",
      "Epoch [2/10], Iter [91/45] Loss: 2.7999\n",
      "Epoch [2/10], Iter [92/45] Loss: 2.7803\n",
      "Epoch [2/10], Iter [93/45] Loss: 2.3312\n",
      "Epoch [2/10], Iter [94/45] Loss: 3.2139\n",
      "Epoch [2/10], Iter [95/45] Loss: 3.1230\n",
      "Epoch [2/10], Iter [96/45] Loss: 2.7481\n",
      "Epoch [2/10], Iter [97/45] Loss: 3.5201\n",
      "Epoch [2/10], Iter [98/45] Loss: 2.7671\n",
      "Epoch [2/10], Iter [99/45] Loss: 2.8459\n",
      "Epoch [2/10], Iter [100/45] Loss: 2.8729\n",
      "Epoch [2/10], Iter [101/45] Loss: 3.0670\n",
      "Epoch [2/10], Iter [102/45] Loss: 2.5801\n",
      "Epoch [2/10], Iter [103/45] Loss: 2.9206\n",
      "Epoch [2/10], Iter [104/45] Loss: 3.0179\n",
      "Epoch [2/10], Iter [105/45] Loss: 2.6834\n",
      "Epoch [2/10], Iter [106/45] Loss: 2.9614\n",
      "Epoch [2/10], Iter [107/45] Loss: 3.0575\n",
      "Epoch [2/10], Iter [108/45] Loss: 2.7438\n",
      "Epoch [2/10], Iter [109/45] Loss: 2.4629\n",
      "Epoch [2/10], Iter [110/45] Loss: 3.0093\n",
      "Epoch [2/10], Iter [111/45] Loss: 2.2994\n",
      "Epoch [2/10], Iter [112/45] Loss: 2.7526\n",
      "Epoch [2/10], Iter [113/45] Loss: 2.6136\n",
      "Epoch [2/10], Iter [114/45] Loss: 2.7297\n",
      "Epoch [2/10], Iter [115/45] Loss: 3.3406\n",
      "Epoch [2/10], Iter [116/45] Loss: 3.0179\n",
      "Epoch [2/10], Iter [117/45] Loss: 2.2320\n",
      "Epoch [2/10], Iter [118/45] Loss: 2.9169\n",
      "Epoch [2/10], Iter [119/45] Loss: 2.2764\n",
      "Epoch [2/10], Iter [120/45] Loss: 2.5609\n",
      "Epoch [2/10], Iter [121/45] Loss: 3.3142\n",
      "Epoch [2/10], Iter [122/45] Loss: 3.2261\n",
      "Epoch [2/10], Iter [123/45] Loss: 2.6499\n",
      "Epoch [2/10], Iter [124/45] Loss: 2.9673\n",
      "Epoch [2/10], Iter [125/45] Loss: 2.6510\n",
      "Epoch [2/10], Iter [126/45] Loss: 2.4627\n",
      "Epoch [2/10], Iter [127/45] Loss: 2.9661\n",
      "Epoch [2/10], Iter [128/45] Loss: 2.5247\n",
      "Epoch [2/10], Iter [129/45] Loss: 2.4987\n",
      "Epoch [2/10], Iter [130/45] Loss: 2.1973\n",
      "Epoch [2/10], Iter [131/45] Loss: 2.3742\n",
      "Epoch [2/10], Iter [132/45] Loss: 2.8816\n",
      "Epoch [2/10], Iter [133/45] Loss: 2.6725\n",
      "Epoch [2/10], Iter [134/45] Loss: 2.6679\n",
      "Epoch [2/10], Iter [135/45] Loss: 2.3473\n",
      "Epoch [2/10], Iter [136/45] Loss: 2.4592\n",
      "Epoch [2/10], Iter [137/45] Loss: 2.7132\n",
      "Epoch [2/10], Iter [138/45] Loss: 3.0060\n",
      "Epoch [2/10], Iter [139/45] Loss: 3.0672\n",
      "Epoch [2/10], Iter [140/45] Loss: 2.7936\n",
      "Epoch [2/10], Iter [141/45] Loss: 2.6891\n",
      "Epoch [2/10], Iter [142/45] Loss: 2.6705\n",
      "Epoch [2/10], Iter [143/45] Loss: 3.1595\n",
      "Epoch [2/10], Iter [144/45] Loss: 2.9638\n",
      "Epoch [2/10], Iter [145/45] Loss: 2.7956\n",
      "Epoch [2/10], Iter [146/45] Loss: 2.3161\n",
      "Epoch [2/10], Iter [147/45] Loss: 2.4195\n",
      "Epoch [2/10], Iter [148/45] Loss: 2.1459\n",
      "Epoch [2/10], Iter [149/45] Loss: 2.0948\n",
      "Epoch [2/10], Iter [150/45] Loss: 2.5915\n",
      "Epoch [2/10], Iter [151/45] Loss: 2.1332\n",
      "Epoch [2/10], Iter [152/45] Loss: 2.8992\n",
      "Epoch [2/10], Iter [153/45] Loss: 2.8678\n",
      "Epoch [2/10], Iter [154/45] Loss: 2.1113\n",
      "Epoch [2/10], Iter [155/45] Loss: 2.9452\n",
      "Epoch [2/10], Iter [156/45] Loss: 2.4700\n",
      "Epoch [2/10], Iter [157/45] Loss: 3.1299\n",
      "Epoch [2/10], Iter [158/45] Loss: 2.3365\n",
      "Epoch [2/10], Iter [159/45] Loss: 2.0183\n",
      "Epoch [2/10], Iter [160/45] Loss: 2.2773\n",
      "Epoch [2/10], Iter [161/45] Loss: 2.0383\n",
      "Epoch [2/10], Iter [162/45] Loss: 2.4393\n",
      "Epoch [2/10], Iter [163/45] Loss: 2.4458\n",
      "Epoch [2/10], Iter [164/45] Loss: 2.3251\n",
      "Epoch [2/10], Iter [165/45] Loss: 2.3553\n",
      "Epoch [2/10], Iter [166/45] Loss: 2.4152\n",
      "Epoch [2/10], Iter [167/45] Loss: 2.4324\n",
      "Epoch [2/10], Iter [168/45] Loss: 2.6475\n",
      "Epoch [2/10], Iter [169/45] Loss: 2.1303\n",
      "Epoch [2/10], Iter [170/45] Loss: 2.0666\n",
      "Epoch [2/10], Iter [171/45] Loss: 2.2626\n",
      "Epoch [2/10], Iter [172/45] Loss: 2.5384\n",
      "Epoch [2/10], Iter [173/45] Loss: 3.0030\n",
      "Epoch [2/10], Iter [174/45] Loss: 2.7448\n",
      "Epoch [2/10], Iter [175/45] Loss: 2.9558\n",
      "Epoch [2/10], Iter [176/45] Loss: 2.4045\n",
      "Epoch [2/10], Iter [177/45] Loss: 2.6663\n",
      "Epoch [2/10], Iter [178/45] Loss: 2.3336\n",
      "Epoch [2/10], Iter [179/45] Loss: 2.5532\n",
      "Epoch [2/10], Iter [180/45] Loss: 2.7577\n",
      "2\n",
      "2\n",
      "Epoch [3/10], Iter [1/45] Loss: 2.3388\n",
      "Epoch [3/10], Iter [2/45] Loss: 1.9807\n",
      "Epoch [3/10], Iter [3/45] Loss: 2.8189\n",
      "Epoch [3/10], Iter [4/45] Loss: 2.8630\n",
      "Epoch [3/10], Iter [5/45] Loss: 1.8648\n",
      "Epoch [3/10], Iter [6/45] Loss: 2.0315\n",
      "Epoch [3/10], Iter [7/45] Loss: 2.2913\n",
      "Epoch [3/10], Iter [8/45] Loss: 2.6461\n",
      "Epoch [3/10], Iter [9/45] Loss: 1.7794\n",
      "Epoch [3/10], Iter [10/45] Loss: 1.8342\n",
      "Epoch [3/10], Iter [11/45] Loss: 2.8553\n",
      "Epoch [3/10], Iter [12/45] Loss: 2.1910\n",
      "Epoch [3/10], Iter [13/45] Loss: 1.9782\n",
      "Epoch [3/10], Iter [14/45] Loss: 2.0272\n",
      "Epoch [3/10], Iter [15/45] Loss: 2.2873\n",
      "Epoch [3/10], Iter [16/45] Loss: 2.0099\n",
      "Epoch [3/10], Iter [17/45] Loss: 1.9408\n",
      "Epoch [3/10], Iter [18/45] Loss: 2.1237\n",
      "Epoch [3/10], Iter [19/45] Loss: 2.2663\n",
      "Epoch [3/10], Iter [20/45] Loss: 2.7211\n",
      "Epoch [3/10], Iter [21/45] Loss: 2.0738\n",
      "Epoch [3/10], Iter [22/45] Loss: 2.1575\n",
      "Epoch [3/10], Iter [23/45] Loss: 2.5368\n",
      "Epoch [3/10], Iter [24/45] Loss: 2.4505\n",
      "Epoch [3/10], Iter [25/45] Loss: 3.0058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Iter [26/45] Loss: 2.3074\n",
      "Epoch [3/10], Iter [27/45] Loss: 1.4880\n",
      "Epoch [3/10], Iter [28/45] Loss: 2.2929\n",
      "Epoch [3/10], Iter [29/45] Loss: 2.0719\n",
      "Epoch [3/10], Iter [30/45] Loss: 2.3234\n",
      "Epoch [3/10], Iter [31/45] Loss: 2.1333\n",
      "Epoch [3/10], Iter [32/45] Loss: 1.8710\n",
      "Epoch [3/10], Iter [33/45] Loss: 1.8589\n",
      "Epoch [3/10], Iter [34/45] Loss: 2.4880\n",
      "Epoch [3/10], Iter [35/45] Loss: 1.8027\n",
      "Epoch [3/10], Iter [36/45] Loss: 2.2283\n",
      "Epoch [3/10], Iter [37/45] Loss: 2.7954\n",
      "Epoch [3/10], Iter [38/45] Loss: 1.9813\n",
      "Epoch [3/10], Iter [39/45] Loss: 1.9576\n",
      "Epoch [3/10], Iter [40/45] Loss: 2.1182\n",
      "Epoch [3/10], Iter [41/45] Loss: 2.1625\n",
      "Epoch [3/10], Iter [42/45] Loss: 2.1481\n",
      "Epoch [3/10], Iter [43/45] Loss: 1.6404\n",
      "Epoch [3/10], Iter [44/45] Loss: 2.1808\n",
      "Epoch [3/10], Iter [45/45] Loss: 2.1951\n",
      "Epoch [3/10], Iter [46/45] Loss: 1.7649\n",
      "Epoch [3/10], Iter [47/45] Loss: 2.0981\n",
      "Epoch [3/10], Iter [48/45] Loss: 1.7492\n",
      "Epoch [3/10], Iter [49/45] Loss: 1.9659\n",
      "Epoch [3/10], Iter [50/45] Loss: 1.4675\n",
      "Epoch [3/10], Iter [51/45] Loss: 1.9389\n",
      "Epoch [3/10], Iter [52/45] Loss: 2.0909\n",
      "Epoch [3/10], Iter [53/45] Loss: 2.0133\n",
      "Epoch [3/10], Iter [54/45] Loss: 2.1748\n",
      "Epoch [3/10], Iter [55/45] Loss: 2.5705\n",
      "Epoch [3/10], Iter [56/45] Loss: 2.0813\n",
      "Epoch [3/10], Iter [57/45] Loss: 1.7993\n",
      "Epoch [3/10], Iter [58/45] Loss: 1.9604\n",
      "Epoch [3/10], Iter [59/45] Loss: 1.7936\n",
      "Epoch [3/10], Iter [60/45] Loss: 2.2924\n",
      "Epoch [3/10], Iter [61/45] Loss: 1.9361\n",
      "Epoch [3/10], Iter [62/45] Loss: 1.8443\n",
      "Epoch [3/10], Iter [63/45] Loss: 1.6630\n",
      "Epoch [3/10], Iter [64/45] Loss: 1.9570\n",
      "Epoch [3/10], Iter [65/45] Loss: 1.9200\n",
      "Epoch [3/10], Iter [66/45] Loss: 2.0128\n",
      "Epoch [3/10], Iter [67/45] Loss: 1.6856\n",
      "Epoch [3/10], Iter [68/45] Loss: 1.4087\n",
      "Epoch [3/10], Iter [69/45] Loss: 1.6656\n",
      "Epoch [3/10], Iter [70/45] Loss: 2.1550\n",
      "Epoch [3/10], Iter [71/45] Loss: 2.4207\n",
      "Epoch [3/10], Iter [72/45] Loss: 2.1018\n",
      "Epoch [3/10], Iter [73/45] Loss: 2.0111\n",
      "Epoch [3/10], Iter [74/45] Loss: 2.3317\n",
      "Epoch [3/10], Iter [75/45] Loss: 2.3291\n",
      "Epoch [3/10], Iter [76/45] Loss: 1.8110\n",
      "Epoch [3/10], Iter [77/45] Loss: 1.9658\n",
      "Epoch [3/10], Iter [78/45] Loss: 1.9698\n",
      "Epoch [3/10], Iter [79/45] Loss: 1.4809\n",
      "Epoch [3/10], Iter [80/45] Loss: 2.0532\n",
      "Epoch [3/10], Iter [81/45] Loss: 1.7866\n",
      "Epoch [3/10], Iter [82/45] Loss: 1.6824\n",
      "Epoch [3/10], Iter [83/45] Loss: 2.0479\n",
      "Epoch [3/10], Iter [84/45] Loss: 2.0619\n",
      "Epoch [3/10], Iter [85/45] Loss: 1.5444\n",
      "Epoch [3/10], Iter [86/45] Loss: 1.8429\n",
      "Epoch [3/10], Iter [87/45] Loss: 1.7404\n",
      "Epoch [3/10], Iter [88/45] Loss: 1.7642\n",
      "Epoch [3/10], Iter [89/45] Loss: 1.6513\n",
      "Epoch [3/10], Iter [90/45] Loss: 1.9528\n",
      "Epoch [3/10], Iter [91/45] Loss: 1.4147\n",
      "Epoch [3/10], Iter [92/45] Loss: 1.9391\n",
      "Epoch [3/10], Iter [93/45] Loss: 1.4745\n",
      "Epoch [3/10], Iter [94/45] Loss: 1.7316\n",
      "Epoch [3/10], Iter [95/45] Loss: 1.5299\n",
      "Epoch [3/10], Iter [96/45] Loss: 1.8502\n",
      "Epoch [3/10], Iter [97/45] Loss: 2.0637\n",
      "Epoch [3/10], Iter [98/45] Loss: 1.1975\n",
      "Epoch [3/10], Iter [99/45] Loss: 1.2825\n",
      "Epoch [3/10], Iter [100/45] Loss: 1.9375\n",
      "Epoch [3/10], Iter [101/45] Loss: 1.4597\n",
      "Epoch [3/10], Iter [102/45] Loss: 1.7862\n",
      "Epoch [3/10], Iter [103/45] Loss: 1.6328\n",
      "Epoch [3/10], Iter [104/45] Loss: 2.1333\n",
      "Epoch [3/10], Iter [105/45] Loss: 1.9368\n",
      "Epoch [3/10], Iter [106/45] Loss: 1.8589\n",
      "Epoch [3/10], Iter [107/45] Loss: 1.5551\n",
      "Epoch [3/10], Iter [108/45] Loss: 1.4761\n",
      "Epoch [3/10], Iter [109/45] Loss: 1.5307\n",
      "Epoch [3/10], Iter [110/45] Loss: 1.6457\n",
      "Epoch [3/10], Iter [111/45] Loss: 1.5798\n",
      "Epoch [3/10], Iter [112/45] Loss: 1.6313\n",
      "Epoch [3/10], Iter [113/45] Loss: 1.1445\n",
      "Epoch [3/10], Iter [114/45] Loss: 1.3237\n",
      "Epoch [3/10], Iter [115/45] Loss: 1.8612\n",
      "Epoch [3/10], Iter [116/45] Loss: 1.8018\n",
      "Epoch [3/10], Iter [117/45] Loss: 1.4273\n",
      "Epoch [3/10], Iter [118/45] Loss: 1.5219\n",
      "Epoch [3/10], Iter [119/45] Loss: 1.7594\n",
      "Epoch [3/10], Iter [120/45] Loss: 1.6603\n",
      "Epoch [3/10], Iter [121/45] Loss: 1.4170\n",
      "Epoch [3/10], Iter [122/45] Loss: 1.3537\n",
      "Epoch [3/10], Iter [123/45] Loss: 1.4222\n",
      "Epoch [3/10], Iter [124/45] Loss: 1.5123\n",
      "Epoch [3/10], Iter [125/45] Loss: 1.2944\n",
      "Epoch [3/10], Iter [126/45] Loss: 1.1760\n",
      "Epoch [3/10], Iter [127/45] Loss: 1.6545\n",
      "Epoch [3/10], Iter [128/45] Loss: 1.6540\n",
      "Epoch [3/10], Iter [129/45] Loss: 2.1078\n",
      "Epoch [3/10], Iter [130/45] Loss: 1.3546\n",
      "Epoch [3/10], Iter [131/45] Loss: 1.6832\n",
      "Epoch [3/10], Iter [132/45] Loss: 1.4014\n",
      "Epoch [3/10], Iter [133/45] Loss: 1.9622\n",
      "Epoch [3/10], Iter [134/45] Loss: 1.8027\n",
      "Epoch [3/10], Iter [135/45] Loss: 1.5855\n",
      "Epoch [3/10], Iter [136/45] Loss: 1.4957\n",
      "Epoch [3/10], Iter [137/45] Loss: 1.0602\n",
      "Epoch [3/10], Iter [138/45] Loss: 1.6185\n",
      "Epoch [3/10], Iter [139/45] Loss: 1.7718\n",
      "Epoch [3/10], Iter [140/45] Loss: 1.4955\n",
      "Epoch [3/10], Iter [141/45] Loss: 1.5953\n",
      "Epoch [3/10], Iter [142/45] Loss: 1.6256\n",
      "Epoch [3/10], Iter [143/45] Loss: 1.1737\n",
      "Epoch [3/10], Iter [144/45] Loss: 1.6839\n",
      "Epoch [3/10], Iter [145/45] Loss: 1.8471\n",
      "Epoch [3/10], Iter [146/45] Loss: 1.3243\n",
      "Epoch [3/10], Iter [147/45] Loss: 1.0491\n",
      "Epoch [3/10], Iter [148/45] Loss: 1.3753\n",
      "Epoch [3/10], Iter [149/45] Loss: 1.7887\n",
      "Epoch [3/10], Iter [150/45] Loss: 1.4826\n",
      "Epoch [3/10], Iter [151/45] Loss: 1.8641\n",
      "Epoch [3/10], Iter [152/45] Loss: 1.3325\n",
      "Epoch [3/10], Iter [153/45] Loss: 0.9076\n",
      "Epoch [3/10], Iter [154/45] Loss: 1.2952\n",
      "Epoch [3/10], Iter [155/45] Loss: 1.2909\n",
      "Epoch [3/10], Iter [156/45] Loss: 1.2386\n",
      "Epoch [3/10], Iter [157/45] Loss: 1.8148\n",
      "Epoch [3/10], Iter [158/45] Loss: 1.8234\n",
      "Epoch [3/10], Iter [159/45] Loss: 1.7223\n",
      "Epoch [3/10], Iter [160/45] Loss: 1.2402\n",
      "Epoch [3/10], Iter [161/45] Loss: 1.1929\n",
      "Epoch [3/10], Iter [162/45] Loss: 1.0411\n",
      "Epoch [3/10], Iter [163/45] Loss: 1.3454\n",
      "Epoch [3/10], Iter [164/45] Loss: 1.5631\n",
      "Epoch [3/10], Iter [165/45] Loss: 1.6459\n",
      "Epoch [3/10], Iter [166/45] Loss: 1.4457\n",
      "Epoch [3/10], Iter [167/45] Loss: 1.6195\n",
      "Epoch [3/10], Iter [168/45] Loss: 1.3291\n",
      "Epoch [3/10], Iter [169/45] Loss: 1.5323\n",
      "Epoch [3/10], Iter [170/45] Loss: 1.3870\n",
      "Epoch [3/10], Iter [171/45] Loss: 1.1961\n",
      "Epoch [3/10], Iter [172/45] Loss: 1.1858\n",
      "Epoch [3/10], Iter [173/45] Loss: 1.7808\n",
      "Epoch [3/10], Iter [174/45] Loss: 1.5821\n",
      "Epoch [3/10], Iter [175/45] Loss: 1.6111\n",
      "Epoch [3/10], Iter [176/45] Loss: 1.1742\n",
      "Epoch [3/10], Iter [177/45] Loss: 1.7307\n",
      "Epoch [3/10], Iter [178/45] Loss: 1.1484\n",
      "Epoch [3/10], Iter [179/45] Loss: 1.1113\n",
      "Epoch [3/10], Iter [180/45] Loss: 1.3267\n",
      "3\n",
      "3\n",
      "Epoch [4/10], Iter [1/45] Loss: 1.8169\n",
      "Epoch [4/10], Iter [2/45] Loss: 1.4419\n",
      "Epoch [4/10], Iter [3/45] Loss: 1.6650\n",
      "Epoch [4/10], Iter [4/45] Loss: 1.5247\n",
      "Epoch [4/10], Iter [5/45] Loss: 1.4975\n",
      "Epoch [4/10], Iter [6/45] Loss: 1.4966\n",
      "Epoch [4/10], Iter [7/45] Loss: 1.8532\n",
      "Epoch [4/10], Iter [8/45] Loss: 1.6899\n",
      "Epoch [4/10], Iter [9/45] Loss: 1.3204\n",
      "Epoch [4/10], Iter [10/45] Loss: 1.0306\n",
      "Epoch [4/10], Iter [11/45] Loss: 0.8847\n",
      "Epoch [4/10], Iter [12/45] Loss: 1.8317\n",
      "Epoch [4/10], Iter [13/45] Loss: 1.3286\n",
      "Epoch [4/10], Iter [14/45] Loss: 1.2484\n",
      "Epoch [4/10], Iter [15/45] Loss: 1.2687\n",
      "Epoch [4/10], Iter [16/45] Loss: 1.0484\n",
      "Epoch [4/10], Iter [17/45] Loss: 1.1613\n",
      "Epoch [4/10], Iter [18/45] Loss: 1.4026\n",
      "Epoch [4/10], Iter [19/45] Loss: 0.9386\n",
      "Epoch [4/10], Iter [20/45] Loss: 1.4435\n",
      "Epoch [4/10], Iter [21/45] Loss: 1.0825\n",
      "Epoch [4/10], Iter [22/45] Loss: 1.2429\n",
      "Epoch [4/10], Iter [23/45] Loss: 1.4493\n",
      "Epoch [4/10], Iter [24/45] Loss: 1.2696\n",
      "Epoch [4/10], Iter [25/45] Loss: 1.2472\n",
      "Epoch [4/10], Iter [26/45] Loss: 1.8820\n",
      "Epoch [4/10], Iter [27/45] Loss: 1.2248\n",
      "Epoch [4/10], Iter [28/45] Loss: 1.5652\n",
      "Epoch [4/10], Iter [29/45] Loss: 1.5304\n",
      "Epoch [4/10], Iter [30/45] Loss: 1.7012\n",
      "Epoch [4/10], Iter [31/45] Loss: 1.5056\n",
      "Epoch [4/10], Iter [32/45] Loss: 1.0689\n",
      "Epoch [4/10], Iter [33/45] Loss: 1.4680\n",
      "Epoch [4/10], Iter [34/45] Loss: 1.1409\n",
      "Epoch [4/10], Iter [35/45] Loss: 1.0281\n",
      "Epoch [4/10], Iter [36/45] Loss: 1.2725\n",
      "Epoch [4/10], Iter [37/45] Loss: 1.2434\n",
      "Epoch [4/10], Iter [38/45] Loss: 1.1923\n",
      "Epoch [4/10], Iter [39/45] Loss: 1.9402\n",
      "Epoch [4/10], Iter [40/45] Loss: 1.0614\n",
      "Epoch [4/10], Iter [41/45] Loss: 0.9279\n",
      "Epoch [4/10], Iter [42/45] Loss: 1.7728\n",
      "Epoch [4/10], Iter [43/45] Loss: 1.0966\n",
      "Epoch [4/10], Iter [44/45] Loss: 1.2701\n",
      "Epoch [4/10], Iter [45/45] Loss: 0.9777\n",
      "Epoch [4/10], Iter [46/45] Loss: 1.7765\n",
      "Epoch [4/10], Iter [47/45] Loss: 1.2625\n",
      "Epoch [4/10], Iter [48/45] Loss: 1.4547\n",
      "Epoch [4/10], Iter [49/45] Loss: 1.6665\n",
      "Epoch [4/10], Iter [50/45] Loss: 1.6729\n",
      "Epoch [4/10], Iter [51/45] Loss: 1.4197\n",
      "Epoch [4/10], Iter [52/45] Loss: 1.1620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Iter [53/45] Loss: 1.2157\n",
      "Epoch [4/10], Iter [54/45] Loss: 1.5978\n",
      "Epoch [4/10], Iter [55/45] Loss: 1.0821\n",
      "Epoch [4/10], Iter [56/45] Loss: 1.4426\n",
      "Epoch [4/10], Iter [57/45] Loss: 1.5315\n",
      "Epoch [4/10], Iter [58/45] Loss: 1.1934\n",
      "Epoch [4/10], Iter [59/45] Loss: 1.5886\n",
      "Epoch [4/10], Iter [60/45] Loss: 2.2167\n",
      "Epoch [4/10], Iter [61/45] Loss: 1.1368\n",
      "Epoch [4/10], Iter [62/45] Loss: 1.4649\n",
      "Epoch [4/10], Iter [63/45] Loss: 1.5248\n",
      "Epoch [4/10], Iter [64/45] Loss: 1.3458\n",
      "Epoch [4/10], Iter [65/45] Loss: 1.1595\n",
      "Epoch [4/10], Iter [66/45] Loss: 1.4534\n",
      "Epoch [4/10], Iter [67/45] Loss: 1.3609\n",
      "Epoch [4/10], Iter [68/45] Loss: 1.2544\n",
      "Epoch [4/10], Iter [69/45] Loss: 1.1531\n",
      "Epoch [4/10], Iter [70/45] Loss: 1.7273\n",
      "Epoch [4/10], Iter [71/45] Loss: 1.5565\n",
      "Epoch [4/10], Iter [72/45] Loss: 1.2288\n",
      "Epoch [4/10], Iter [73/45] Loss: 1.6205\n",
      "Epoch [4/10], Iter [74/45] Loss: 1.2039\n",
      "Epoch [4/10], Iter [75/45] Loss: 1.4314\n",
      "Epoch [4/10], Iter [76/45] Loss: 1.2746\n",
      "Epoch [4/10], Iter [77/45] Loss: 1.5261\n",
      "Epoch [4/10], Iter [78/45] Loss: 1.1446\n",
      "Epoch [4/10], Iter [79/45] Loss: 0.9745\n",
      "Epoch [4/10], Iter [80/45] Loss: 1.2924\n",
      "Epoch [4/10], Iter [81/45] Loss: 0.9848\n",
      "Epoch [4/10], Iter [82/45] Loss: 1.4895\n",
      "Epoch [4/10], Iter [83/45] Loss: 1.4456\n",
      "Epoch [4/10], Iter [84/45] Loss: 1.0918\n",
      "Epoch [4/10], Iter [85/45] Loss: 1.1693\n",
      "Epoch [4/10], Iter [86/45] Loss: 1.2904\n",
      "Epoch [4/10], Iter [87/45] Loss: 1.3392\n",
      "Epoch [4/10], Iter [88/45] Loss: 1.0597\n",
      "Epoch [4/10], Iter [89/45] Loss: 1.1053\n",
      "Epoch [4/10], Iter [90/45] Loss: 0.9590\n",
      "Epoch [4/10], Iter [91/45] Loss: 0.9540\n",
      "Epoch [4/10], Iter [92/45] Loss: 0.9080\n",
      "Epoch [4/10], Iter [93/45] Loss: 1.2008\n",
      "Epoch [4/10], Iter [94/45] Loss: 1.3668\n",
      "Epoch [4/10], Iter [95/45] Loss: 1.0147\n",
      "Epoch [4/10], Iter [96/45] Loss: 1.5143\n",
      "Epoch [4/10], Iter [97/45] Loss: 1.2348\n",
      "Epoch [4/10], Iter [98/45] Loss: 1.1382\n",
      "Epoch [4/10], Iter [99/45] Loss: 1.3434\n",
      "Epoch [4/10], Iter [100/45] Loss: 1.5042\n",
      "Epoch [4/10], Iter [101/45] Loss: 1.0892\n",
      "Epoch [4/10], Iter [102/45] Loss: 1.2873\n",
      "Epoch [4/10], Iter [103/45] Loss: 0.8503\n",
      "Epoch [4/10], Iter [104/45] Loss: 1.4183\n",
      "Epoch [4/10], Iter [105/45] Loss: 1.4586\n",
      "Epoch [4/10], Iter [106/45] Loss: 1.2874\n",
      "Epoch [4/10], Iter [107/45] Loss: 1.0249\n",
      "Epoch [4/10], Iter [108/45] Loss: 1.3532\n",
      "Epoch [4/10], Iter [109/45] Loss: 1.4360\n",
      "Epoch [4/10], Iter [110/45] Loss: 1.1681\n",
      "Epoch [4/10], Iter [111/45] Loss: 2.0056\n",
      "Epoch [4/10], Iter [112/45] Loss: 1.2097\n",
      "Epoch [4/10], Iter [113/45] Loss: 0.9233\n",
      "Epoch [4/10], Iter [114/45] Loss: 1.4517\n",
      "Epoch [4/10], Iter [115/45] Loss: 1.2894\n",
      "Epoch [4/10], Iter [116/45] Loss: 1.2501\n",
      "Epoch [4/10], Iter [117/45] Loss: 1.2635\n",
      "Epoch [4/10], Iter [118/45] Loss: 1.2097\n",
      "Epoch [4/10], Iter [119/45] Loss: 1.1470\n",
      "Epoch [4/10], Iter [120/45] Loss: 1.0224\n",
      "Epoch [4/10], Iter [121/45] Loss: 1.4730\n",
      "Epoch [4/10], Iter [122/45] Loss: 1.6267\n",
      "Epoch [4/10], Iter [123/45] Loss: 1.0285\n",
      "Epoch [4/10], Iter [124/45] Loss: 1.0586\n",
      "Epoch [4/10], Iter [125/45] Loss: 1.2976\n",
      "Epoch [4/10], Iter [126/45] Loss: 1.2966\n",
      "Epoch [4/10], Iter [127/45] Loss: 1.0206\n",
      "Epoch [4/10], Iter [128/45] Loss: 0.9782\n",
      "Epoch [4/10], Iter [129/45] Loss: 1.1795\n",
      "Epoch [4/10], Iter [130/45] Loss: 1.3453\n",
      "Epoch [4/10], Iter [131/45] Loss: 1.6664\n",
      "Epoch [4/10], Iter [132/45] Loss: 0.8703\n",
      "Epoch [4/10], Iter [133/45] Loss: 1.0412\n",
      "Epoch [4/10], Iter [134/45] Loss: 0.9921\n",
      "Epoch [4/10], Iter [135/45] Loss: 1.3301\n",
      "Epoch [4/10], Iter [136/45] Loss: 0.9315\n",
      "Epoch [4/10], Iter [137/45] Loss: 0.8477\n",
      "Epoch [4/10], Iter [138/45] Loss: 0.8102\n",
      "Epoch [4/10], Iter [139/45] Loss: 1.6426\n",
      "Epoch [4/10], Iter [140/45] Loss: 1.6282\n",
      "Epoch [4/10], Iter [141/45] Loss: 1.5466\n",
      "Epoch [4/10], Iter [142/45] Loss: 1.0986\n",
      "Epoch [4/10], Iter [143/45] Loss: 1.5822\n",
      "Epoch [4/10], Iter [144/45] Loss: 0.9649\n",
      "Epoch [4/10], Iter [145/45] Loss: 0.9439\n",
      "Epoch [4/10], Iter [146/45] Loss: 0.8956\n",
      "Epoch [4/10], Iter [147/45] Loss: 1.7607\n",
      "Epoch [4/10], Iter [148/45] Loss: 1.1391\n",
      "Epoch [4/10], Iter [149/45] Loss: 1.2188\n",
      "Epoch [4/10], Iter [150/45] Loss: 0.9659\n",
      "Epoch [4/10], Iter [151/45] Loss: 1.7150\n",
      "Epoch [4/10], Iter [152/45] Loss: 1.2944\n",
      "Epoch [4/10], Iter [153/45] Loss: 0.9825\n",
      "Epoch [4/10], Iter [154/45] Loss: 1.1606\n",
      "Epoch [4/10], Iter [155/45] Loss: 1.1724\n",
      "Epoch [4/10], Iter [156/45] Loss: 1.3369\n",
      "Epoch [4/10], Iter [157/45] Loss: 0.9370\n",
      "Epoch [4/10], Iter [158/45] Loss: 1.2340\n",
      "Epoch [4/10], Iter [159/45] Loss: 1.0214\n",
      "Epoch [4/10], Iter [160/45] Loss: 1.3199\n",
      "Epoch [4/10], Iter [161/45] Loss: 1.1991\n",
      "Epoch [4/10], Iter [162/45] Loss: 1.0352\n",
      "Epoch [4/10], Iter [163/45] Loss: 1.2387\n",
      "Epoch [4/10], Iter [164/45] Loss: 1.1167\n",
      "Epoch [4/10], Iter [165/45] Loss: 1.1701\n",
      "Epoch [4/10], Iter [166/45] Loss: 1.4486\n",
      "Epoch [4/10], Iter [167/45] Loss: 0.8957\n",
      "Epoch [4/10], Iter [168/45] Loss: 1.1126\n",
      "Epoch [4/10], Iter [169/45] Loss: 1.0676\n",
      "Epoch [4/10], Iter [170/45] Loss: 0.6943\n",
      "Epoch [4/10], Iter [171/45] Loss: 0.9522\n",
      "Epoch [4/10], Iter [172/45] Loss: 1.3017\n",
      "Epoch [4/10], Iter [173/45] Loss: 0.6346\n",
      "Epoch [4/10], Iter [174/45] Loss: 0.9539\n",
      "Epoch [4/10], Iter [175/45] Loss: 1.1118\n",
      "Epoch [4/10], Iter [176/45] Loss: 1.5708\n",
      "Epoch [4/10], Iter [177/45] Loss: 1.0944\n",
      "Epoch [4/10], Iter [178/45] Loss: 0.8716\n",
      "Epoch [4/10], Iter [179/45] Loss: 1.2940\n",
      "Epoch [4/10], Iter [180/45] Loss: 1.6064\n",
      "4\n",
      "4\n",
      "Epoch [5/10], Iter [1/45] Loss: 0.8741\n",
      "Epoch [5/10], Iter [2/45] Loss: 0.9113\n",
      "Epoch [5/10], Iter [3/45] Loss: 1.1135\n",
      "Epoch [5/10], Iter [4/45] Loss: 1.3967\n",
      "Epoch [5/10], Iter [5/45] Loss: 1.2954\n",
      "Epoch [5/10], Iter [6/45] Loss: 0.9290\n",
      "Epoch [5/10], Iter [7/45] Loss: 1.1218\n",
      "Epoch [5/10], Iter [8/45] Loss: 1.9682\n",
      "Epoch [5/10], Iter [9/45] Loss: 1.1012\n",
      "Epoch [5/10], Iter [10/45] Loss: 0.8922\n",
      "Epoch [5/10], Iter [11/45] Loss: 1.2389\n",
      "Epoch [5/10], Iter [12/45] Loss: 1.1885\n",
      "Epoch [5/10], Iter [13/45] Loss: 1.6294\n",
      "Epoch [5/10], Iter [14/45] Loss: 1.6276\n",
      "Epoch [5/10], Iter [15/45] Loss: 1.0173\n",
      "Epoch [5/10], Iter [16/45] Loss: 0.8381\n",
      "Epoch [5/10], Iter [17/45] Loss: 0.7820\n",
      "Epoch [5/10], Iter [18/45] Loss: 1.1359\n",
      "Epoch [5/10], Iter [19/45] Loss: 1.1062\n",
      "Epoch [5/10], Iter [20/45] Loss: 0.7317\n",
      "Epoch [5/10], Iter [21/45] Loss: 1.1853\n",
      "Epoch [5/10], Iter [22/45] Loss: 1.3326\n",
      "Epoch [5/10], Iter [23/45] Loss: 0.7385\n",
      "Epoch [5/10], Iter [24/45] Loss: 1.4061\n",
      "Epoch [5/10], Iter [25/45] Loss: 1.1290\n",
      "Epoch [5/10], Iter [26/45] Loss: 1.0609\n",
      "Epoch [5/10], Iter [27/45] Loss: 1.0301\n",
      "Epoch [5/10], Iter [28/45] Loss: 1.3288\n",
      "Epoch [5/10], Iter [29/45] Loss: 1.5658\n",
      "Epoch [5/10], Iter [30/45] Loss: 0.8053\n",
      "Epoch [5/10], Iter [31/45] Loss: 0.6556\n",
      "Epoch [5/10], Iter [32/45] Loss: 1.0193\n",
      "Epoch [5/10], Iter [33/45] Loss: 1.1498\n",
      "Epoch [5/10], Iter [34/45] Loss: 1.2377\n",
      "Epoch [5/10], Iter [35/45] Loss: 1.2190\n",
      "Epoch [5/10], Iter [36/45] Loss: 1.2342\n",
      "Epoch [5/10], Iter [37/45] Loss: 1.2896\n",
      "Epoch [5/10], Iter [38/45] Loss: 0.8645\n",
      "Epoch [5/10], Iter [39/45] Loss: 0.8378\n",
      "Epoch [5/10], Iter [40/45] Loss: 0.8862\n",
      "Epoch [5/10], Iter [41/45] Loss: 1.3120\n",
      "Epoch [5/10], Iter [42/45] Loss: 1.6967\n",
      "Epoch [5/10], Iter [43/45] Loss: 0.8182\n",
      "Epoch [5/10], Iter [44/45] Loss: 1.3008\n",
      "Epoch [5/10], Iter [45/45] Loss: 0.7476\n",
      "Epoch [5/10], Iter [46/45] Loss: 1.6416\n",
      "Epoch [5/10], Iter [47/45] Loss: 0.6150\n",
      "Epoch [5/10], Iter [48/45] Loss: 1.1399\n",
      "Epoch [5/10], Iter [49/45] Loss: 1.4557\n",
      "Epoch [5/10], Iter [50/45] Loss: 0.8163\n",
      "Epoch [5/10], Iter [51/45] Loss: 0.8552\n",
      "Epoch [5/10], Iter [52/45] Loss: 1.0642\n",
      "Epoch [5/10], Iter [53/45] Loss: 1.0942\n",
      "Epoch [5/10], Iter [54/45] Loss: 1.0569\n",
      "Epoch [5/10], Iter [55/45] Loss: 0.9295\n",
      "Epoch [5/10], Iter [56/45] Loss: 1.6600\n",
      "Epoch [5/10], Iter [57/45] Loss: 0.8073\n",
      "Epoch [5/10], Iter [58/45] Loss: 1.2055\n",
      "Epoch [5/10], Iter [59/45] Loss: 0.7957\n",
      "Epoch [5/10], Iter [60/45] Loss: 1.0972\n",
      "Epoch [5/10], Iter [61/45] Loss: 1.1271\n",
      "Epoch [5/10], Iter [62/45] Loss: 1.0256\n",
      "Epoch [5/10], Iter [63/45] Loss: 1.2664\n",
      "Epoch [5/10], Iter [64/45] Loss: 0.8319\n",
      "Epoch [5/10], Iter [65/45] Loss: 1.4400\n",
      "Epoch [5/10], Iter [66/45] Loss: 1.3932\n",
      "Epoch [5/10], Iter [67/45] Loss: 1.3577\n",
      "Epoch [5/10], Iter [68/45] Loss: 1.5598\n",
      "Epoch [5/10], Iter [69/45] Loss: 1.3284\n",
      "Epoch [5/10], Iter [70/45] Loss: 0.9556\n",
      "Epoch [5/10], Iter [71/45] Loss: 1.0819\n",
      "Epoch [5/10], Iter [72/45] Loss: 0.8735\n",
      "Epoch [5/10], Iter [73/45] Loss: 1.2849\n",
      "Epoch [5/10], Iter [74/45] Loss: 1.4123\n",
      "Epoch [5/10], Iter [75/45] Loss: 1.0549\n",
      "Epoch [5/10], Iter [76/45] Loss: 1.1122\n",
      "Epoch [5/10], Iter [77/45] Loss: 1.1266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Iter [78/45] Loss: 0.8375\n",
      "Epoch [5/10], Iter [79/45] Loss: 1.0350\n",
      "Epoch [5/10], Iter [80/45] Loss: 1.0871\n",
      "Epoch [5/10], Iter [81/45] Loss: 1.0194\n",
      "Epoch [5/10], Iter [82/45] Loss: 1.3046\n",
      "Epoch [5/10], Iter [83/45] Loss: 1.5095\n",
      "Epoch [5/10], Iter [84/45] Loss: 0.8551\n",
      "Epoch [5/10], Iter [85/45] Loss: 0.8923\n",
      "Epoch [5/10], Iter [86/45] Loss: 0.8276\n",
      "Epoch [5/10], Iter [87/45] Loss: 0.4028\n",
      "Epoch [5/10], Iter [88/45] Loss: 1.1220\n",
      "Epoch [5/10], Iter [89/45] Loss: 0.5337\n",
      "Epoch [5/10], Iter [90/45] Loss: 1.7640\n",
      "Epoch [5/10], Iter [91/45] Loss: 1.2368\n",
      "Epoch [5/10], Iter [92/45] Loss: 1.4646\n",
      "Epoch [5/10], Iter [93/45] Loss: 1.3174\n",
      "Epoch [5/10], Iter [94/45] Loss: 1.2687\n",
      "Epoch [5/10], Iter [95/45] Loss: 0.9380\n",
      "Epoch [5/10], Iter [96/45] Loss: 1.4729\n",
      "Epoch [5/10], Iter [97/45] Loss: 1.1471\n",
      "Epoch [5/10], Iter [98/45] Loss: 1.4018\n",
      "Epoch [5/10], Iter [99/45] Loss: 0.7121\n",
      "Epoch [5/10], Iter [100/45] Loss: 1.0853\n",
      "Epoch [5/10], Iter [101/45] Loss: 1.1766\n",
      "Epoch [5/10], Iter [102/45] Loss: 1.0508\n",
      "Epoch [5/10], Iter [103/45] Loss: 0.7873\n",
      "Epoch [5/10], Iter [104/45] Loss: 0.6682\n",
      "Epoch [5/10], Iter [105/45] Loss: 1.0334\n",
      "Epoch [5/10], Iter [106/45] Loss: 1.0357\n",
      "Epoch [5/10], Iter [107/45] Loss: 1.0959\n",
      "Epoch [5/10], Iter [108/45] Loss: 0.7575\n",
      "Epoch [5/10], Iter [109/45] Loss: 1.2358\n",
      "Epoch [5/10], Iter [110/45] Loss: 1.4659\n",
      "Epoch [5/10], Iter [111/45] Loss: 0.7994\n",
      "Epoch [5/10], Iter [112/45] Loss: 1.0559\n",
      "Epoch [5/10], Iter [113/45] Loss: 1.0110\n",
      "Epoch [5/10], Iter [114/45] Loss: 0.9245\n",
      "Epoch [5/10], Iter [115/45] Loss: 0.8001\n",
      "Epoch [5/10], Iter [116/45] Loss: 0.8588\n",
      "Epoch [5/10], Iter [117/45] Loss: 1.1627\n",
      "Epoch [5/10], Iter [118/45] Loss: 1.2881\n",
      "Epoch [5/10], Iter [119/45] Loss: 0.8035\n",
      "Epoch [5/10], Iter [120/45] Loss: 1.4122\n",
      "Epoch [5/10], Iter [121/45] Loss: 1.1623\n",
      "Epoch [5/10], Iter [122/45] Loss: 0.9461\n",
      "Epoch [5/10], Iter [123/45] Loss: 1.2248\n",
      "Epoch [5/10], Iter [124/45] Loss: 0.6089\n",
      "Epoch [5/10], Iter [125/45] Loss: 1.6384\n",
      "Epoch [5/10], Iter [126/45] Loss: 1.2983\n",
      "Epoch [5/10], Iter [127/45] Loss: 1.4169\n",
      "Epoch [5/10], Iter [128/45] Loss: 0.9732\n",
      "Epoch [5/10], Iter [129/45] Loss: 1.3002\n",
      "Epoch [5/10], Iter [130/45] Loss: 1.4075\n",
      "Epoch [5/10], Iter [131/45] Loss: 1.4380\n",
      "Epoch [5/10], Iter [132/45] Loss: 0.8677\n",
      "Epoch [5/10], Iter [133/45] Loss: 1.1913\n",
      "Epoch [5/10], Iter [134/45] Loss: 0.8396\n",
      "Epoch [5/10], Iter [135/45] Loss: 1.7009\n",
      "Epoch [5/10], Iter [136/45] Loss: 1.2140\n",
      "Epoch [5/10], Iter [137/45] Loss: 1.5007\n",
      "Epoch [5/10], Iter [138/45] Loss: 0.9104\n",
      "Epoch [5/10], Iter [139/45] Loss: 0.9454\n",
      "Epoch [5/10], Iter [140/45] Loss: 1.0622\n",
      "Epoch [5/10], Iter [141/45] Loss: 1.2230\n",
      "Epoch [5/10], Iter [142/45] Loss: 1.0728\n",
      "Epoch [5/10], Iter [143/45] Loss: 1.0207\n",
      "Epoch [5/10], Iter [144/45] Loss: 0.9708\n",
      "Epoch [5/10], Iter [145/45] Loss: 1.2190\n",
      "Epoch [5/10], Iter [146/45] Loss: 0.8395\n",
      "Epoch [5/10], Iter [147/45] Loss: 0.9225\n",
      "Epoch [5/10], Iter [148/45] Loss: 1.0109\n",
      "Epoch [5/10], Iter [149/45] Loss: 1.1133\n",
      "Epoch [5/10], Iter [150/45] Loss: 0.8455\n",
      "Epoch [5/10], Iter [151/45] Loss: 1.2771\n",
      "Epoch [5/10], Iter [152/45] Loss: 1.1494\n",
      "Epoch [5/10], Iter [153/45] Loss: 1.1301\n",
      "Epoch [5/10], Iter [154/45] Loss: 0.6701\n",
      "Epoch [5/10], Iter [155/45] Loss: 0.6066\n",
      "Epoch [5/10], Iter [156/45] Loss: 1.0455\n",
      "Epoch [5/10], Iter [157/45] Loss: 1.3232\n",
      "Epoch [5/10], Iter [158/45] Loss: 1.4645\n",
      "Epoch [5/10], Iter [159/45] Loss: 1.2631\n",
      "Epoch [5/10], Iter [160/45] Loss: 1.1633\n",
      "Epoch [5/10], Iter [161/45] Loss: 1.2305\n",
      "Epoch [5/10], Iter [162/45] Loss: 1.2938\n",
      "Epoch [5/10], Iter [163/45] Loss: 1.2677\n",
      "Epoch [5/10], Iter [164/45] Loss: 1.3829\n",
      "Epoch [5/10], Iter [165/45] Loss: 1.1727\n",
      "Epoch [5/10], Iter [166/45] Loss: 0.8689\n",
      "Epoch [5/10], Iter [167/45] Loss: 1.0659\n",
      "Epoch [5/10], Iter [168/45] Loss: 1.2118\n",
      "Epoch [5/10], Iter [169/45] Loss: 1.2295\n",
      "Epoch [5/10], Iter [170/45] Loss: 1.0508\n",
      "Epoch [5/10], Iter [171/45] Loss: 0.7879\n",
      "Epoch [5/10], Iter [172/45] Loss: 1.0068\n",
      "Epoch [5/10], Iter [173/45] Loss: 1.3793\n",
      "Epoch [5/10], Iter [174/45] Loss: 1.4447\n",
      "Epoch [5/10], Iter [175/45] Loss: 1.0729\n",
      "Epoch [5/10], Iter [176/45] Loss: 0.9092\n",
      "Epoch [5/10], Iter [177/45] Loss: 0.6360\n",
      "Epoch [5/10], Iter [178/45] Loss: 1.2049\n",
      "Epoch [5/10], Iter [179/45] Loss: 0.9869\n",
      "Epoch [5/10], Iter [180/45] Loss: 0.8388\n",
      "5\n",
      "5\n",
      "Epoch [6/10], Iter [1/45] Loss: 0.7318\n",
      "Epoch [6/10], Iter [2/45] Loss: 1.0049\n",
      "Epoch [6/10], Iter [3/45] Loss: 1.0403\n",
      "Epoch [6/10], Iter [4/45] Loss: 0.9957\n",
      "Epoch [6/10], Iter [5/45] Loss: 0.6882\n",
      "Epoch [6/10], Iter [6/45] Loss: 0.5842\n",
      "Epoch [6/10], Iter [7/45] Loss: 1.1906\n",
      "Epoch [6/10], Iter [8/45] Loss: 1.0708\n",
      "Epoch [6/10], Iter [9/45] Loss: 1.1551\n",
      "Epoch [6/10], Iter [10/45] Loss: 1.0336\n",
      "Epoch [6/10], Iter [11/45] Loss: 0.6312\n",
      "Epoch [6/10], Iter [12/45] Loss: 1.1017\n",
      "Epoch [6/10], Iter [13/45] Loss: 1.0232\n",
      "Epoch [6/10], Iter [14/45] Loss: 1.2707\n",
      "Epoch [6/10], Iter [15/45] Loss: 0.9361\n",
      "Epoch [6/10], Iter [16/45] Loss: 1.1859\n",
      "Epoch [6/10], Iter [17/45] Loss: 0.8691\n",
      "Epoch [6/10], Iter [18/45] Loss: 1.0053\n",
      "Epoch [6/10], Iter [19/45] Loss: 1.4144\n",
      "Epoch [6/10], Iter [20/45] Loss: 1.1020\n",
      "Epoch [6/10], Iter [21/45] Loss: 1.3300\n",
      "Epoch [6/10], Iter [22/45] Loss: 1.1816\n",
      "Epoch [6/10], Iter [23/45] Loss: 1.1781\n",
      "Epoch [6/10], Iter [24/45] Loss: 1.3357\n",
      "Epoch [6/10], Iter [25/45] Loss: 1.3577\n",
      "Epoch [6/10], Iter [26/45] Loss: 0.9778\n",
      "Epoch [6/10], Iter [27/45] Loss: 0.8190\n",
      "Epoch [6/10], Iter [28/45] Loss: 0.8040\n",
      "Epoch [6/10], Iter [29/45] Loss: 1.1800\n",
      "Epoch [6/10], Iter [30/45] Loss: 1.5189\n",
      "Epoch [6/10], Iter [31/45] Loss: 0.9631\n",
      "Epoch [6/10], Iter [32/45] Loss: 1.2395\n",
      "Epoch [6/10], Iter [33/45] Loss: 0.9783\n",
      "Epoch [6/10], Iter [34/45] Loss: 1.1994\n",
      "Epoch [6/10], Iter [35/45] Loss: 0.8809\n",
      "Epoch [6/10], Iter [36/45] Loss: 1.4334\n",
      "Epoch [6/10], Iter [37/45] Loss: 0.8979\n",
      "Epoch [6/10], Iter [38/45] Loss: 1.2989\n",
      "Epoch [6/10], Iter [39/45] Loss: 1.3349\n",
      "Epoch [6/10], Iter [40/45] Loss: 1.4753\n",
      "Epoch [6/10], Iter [41/45] Loss: 1.2285\n",
      "Epoch [6/10], Iter [42/45] Loss: 1.1662\n",
      "Epoch [6/10], Iter [43/45] Loss: 1.2206\n",
      "Epoch [6/10], Iter [44/45] Loss: 0.9003\n",
      "Epoch [6/10], Iter [45/45] Loss: 0.7558\n",
      "Epoch [6/10], Iter [46/45] Loss: 1.3013\n",
      "Epoch [6/10], Iter [47/45] Loss: 1.2256\n",
      "Epoch [6/10], Iter [48/45] Loss: 1.0024\n",
      "Epoch [6/10], Iter [49/45] Loss: 0.9497\n",
      "Epoch [6/10], Iter [50/45] Loss: 0.7393\n",
      "Epoch [6/10], Iter [51/45] Loss: 0.7382\n",
      "Epoch [6/10], Iter [52/45] Loss: 1.2870\n",
      "Epoch [6/10], Iter [53/45] Loss: 1.5157\n",
      "Epoch [6/10], Iter [54/45] Loss: 0.7520\n",
      "Epoch [6/10], Iter [55/45] Loss: 0.6833\n",
      "Epoch [6/10], Iter [56/45] Loss: 0.7973\n",
      "Epoch [6/10], Iter [57/45] Loss: 1.0171\n",
      "Epoch [6/10], Iter [58/45] Loss: 0.6218\n",
      "Epoch [6/10], Iter [59/45] Loss: 1.2086\n",
      "Epoch [6/10], Iter [60/45] Loss: 1.0561\n",
      "Epoch [6/10], Iter [61/45] Loss: 1.2209\n",
      "Epoch [6/10], Iter [62/45] Loss: 1.0856\n",
      "Epoch [6/10], Iter [63/45] Loss: 0.9527\n",
      "Epoch [6/10], Iter [64/45] Loss: 1.8061\n",
      "Epoch [6/10], Iter [65/45] Loss: 0.8992\n",
      "Epoch [6/10], Iter [66/45] Loss: 1.3002\n",
      "Epoch [6/10], Iter [67/45] Loss: 1.4197\n",
      "Epoch [6/10], Iter [68/45] Loss: 0.9742\n",
      "Epoch [6/10], Iter [69/45] Loss: 0.9753\n",
      "Epoch [6/10], Iter [70/45] Loss: 0.9457\n",
      "Epoch [6/10], Iter [71/45] Loss: 1.1376\n",
      "Epoch [6/10], Iter [72/45] Loss: 1.0668\n",
      "Epoch [6/10], Iter [73/45] Loss: 0.9808\n",
      "Epoch [6/10], Iter [74/45] Loss: 0.9827\n",
      "Epoch [6/10], Iter [75/45] Loss: 0.6115\n",
      "Epoch [6/10], Iter [76/45] Loss: 0.7423\n",
      "Epoch [6/10], Iter [77/45] Loss: 1.0607\n",
      "Epoch [6/10], Iter [78/45] Loss: 1.0703\n",
      "Epoch [6/10], Iter [79/45] Loss: 1.5526\n",
      "Epoch [6/10], Iter [80/45] Loss: 1.0513\n",
      "Epoch [6/10], Iter [81/45] Loss: 1.0743\n",
      "Epoch [6/10], Iter [82/45] Loss: 0.7649\n",
      "Epoch [6/10], Iter [83/45] Loss: 1.0630\n",
      "Epoch [6/10], Iter [84/45] Loss: 1.1095\n",
      "Epoch [6/10], Iter [85/45] Loss: 1.0916\n",
      "Epoch [6/10], Iter [86/45] Loss: 0.9092\n",
      "Epoch [6/10], Iter [87/45] Loss: 1.1904\n",
      "Epoch [6/10], Iter [88/45] Loss: 0.9489\n",
      "Epoch [6/10], Iter [89/45] Loss: 1.1886\n",
      "Epoch [6/10], Iter [90/45] Loss: 0.9522\n",
      "Epoch [6/10], Iter [91/45] Loss: 0.6107\n",
      "Epoch [6/10], Iter [92/45] Loss: 1.3508\n",
      "Epoch [6/10], Iter [93/45] Loss: 0.9379\n",
      "Epoch [6/10], Iter [94/45] Loss: 0.8327\n",
      "Epoch [6/10], Iter [95/45] Loss: 1.0383\n",
      "Epoch [6/10], Iter [96/45] Loss: 0.7430\n",
      "Epoch [6/10], Iter [97/45] Loss: 0.9559\n",
      "Epoch [6/10], Iter [98/45] Loss: 0.8885\n",
      "Epoch [6/10], Iter [99/45] Loss: 1.3602\n",
      "Epoch [6/10], Iter [100/45] Loss: 0.8860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Iter [101/45] Loss: 0.8855\n",
      "Epoch [6/10], Iter [102/45] Loss: 0.9130\n",
      "Epoch [6/10], Iter [103/45] Loss: 1.3656\n",
      "Epoch [6/10], Iter [104/45] Loss: 1.1948\n",
      "Epoch [6/10], Iter [105/45] Loss: 0.9373\n",
      "Epoch [6/10], Iter [106/45] Loss: 1.1753\n",
      "Epoch [6/10], Iter [107/45] Loss: 0.5749\n",
      "Epoch [6/10], Iter [108/45] Loss: 1.3233\n",
      "Epoch [6/10], Iter [109/45] Loss: 1.3030\n",
      "Epoch [6/10], Iter [110/45] Loss: 0.9674\n",
      "Epoch [6/10], Iter [111/45] Loss: 0.7859\n",
      "Epoch [6/10], Iter [112/45] Loss: 1.3196\n",
      "Epoch [6/10], Iter [113/45] Loss: 1.3463\n",
      "Epoch [6/10], Iter [114/45] Loss: 0.7841\n",
      "Epoch [6/10], Iter [115/45] Loss: 1.6198\n",
      "Epoch [6/10], Iter [116/45] Loss: 0.7480\n",
      "Epoch [6/10], Iter [117/45] Loss: 0.8921\n",
      "Epoch [6/10], Iter [118/45] Loss: 1.2012\n",
      "Epoch [6/10], Iter [119/45] Loss: 1.3271\n",
      "Epoch [6/10], Iter [120/45] Loss: 1.0725\n",
      "Epoch [6/10], Iter [121/45] Loss: 1.0440\n",
      "Epoch [6/10], Iter [122/45] Loss: 1.1987\n",
      "Epoch [6/10], Iter [123/45] Loss: 1.1935\n",
      "Epoch [6/10], Iter [124/45] Loss: 1.3618\n",
      "Epoch [6/10], Iter [125/45] Loss: 0.8001\n",
      "Epoch [6/10], Iter [126/45] Loss: 1.0311\n",
      "Epoch [6/10], Iter [127/45] Loss: 1.4354\n",
      "Epoch [6/10], Iter [128/45] Loss: 0.9042\n",
      "Epoch [6/10], Iter [129/45] Loss: 1.4039\n",
      "Epoch [6/10], Iter [130/45] Loss: 0.8580\n",
      "Epoch [6/10], Iter [131/45] Loss: 0.6869\n",
      "Epoch [6/10], Iter [132/45] Loss: 0.8142\n",
      "Epoch [6/10], Iter [133/45] Loss: 1.8857\n",
      "Epoch [6/10], Iter [134/45] Loss: 1.0320\n",
      "Epoch [6/10], Iter [135/45] Loss: 1.1220\n",
      "Epoch [6/10], Iter [136/45] Loss: 0.8332\n",
      "Epoch [6/10], Iter [137/45] Loss: 0.6329\n",
      "Epoch [6/10], Iter [138/45] Loss: 1.0302\n",
      "Epoch [6/10], Iter [139/45] Loss: 0.9092\n",
      "Epoch [6/10], Iter [140/45] Loss: 1.1500\n",
      "Epoch [6/10], Iter [141/45] Loss: 1.2190\n",
      "Epoch [6/10], Iter [142/45] Loss: 0.9653\n",
      "Epoch [6/10], Iter [143/45] Loss: 0.8288\n",
      "Epoch [6/10], Iter [144/45] Loss: 1.1892\n",
      "Epoch [6/10], Iter [145/45] Loss: 1.1967\n",
      "Epoch [6/10], Iter [146/45] Loss: 1.3217\n",
      "Epoch [6/10], Iter [147/45] Loss: 1.2306\n",
      "Epoch [6/10], Iter [148/45] Loss: 0.9749\n",
      "Epoch [6/10], Iter [149/45] Loss: 0.7563\n",
      "Epoch [6/10], Iter [150/45] Loss: 1.2712\n",
      "Epoch [6/10], Iter [151/45] Loss: 0.8206\n",
      "Epoch [6/10], Iter [152/45] Loss: 0.6123\n",
      "Epoch [6/10], Iter [153/45] Loss: 1.3310\n",
      "Epoch [6/10], Iter [154/45] Loss: 0.9050\n",
      "Epoch [6/10], Iter [155/45] Loss: 1.2622\n",
      "Epoch [6/10], Iter [156/45] Loss: 0.8588\n",
      "Epoch [6/10], Iter [157/45] Loss: 0.5772\n",
      "Epoch [6/10], Iter [158/45] Loss: 1.0287\n",
      "Epoch [6/10], Iter [159/45] Loss: 0.8743\n",
      "Epoch [6/10], Iter [160/45] Loss: 1.2329\n",
      "Epoch [6/10], Iter [161/45] Loss: 1.2043\n",
      "Epoch [6/10], Iter [162/45] Loss: 0.7882\n",
      "Epoch [6/10], Iter [163/45] Loss: 1.0258\n",
      "Epoch [6/10], Iter [164/45] Loss: 1.0519\n",
      "Epoch [6/10], Iter [165/45] Loss: 1.2102\n",
      "Epoch [6/10], Iter [166/45] Loss: 0.9866\n",
      "Epoch [6/10], Iter [167/45] Loss: 1.3191\n",
      "Epoch [6/10], Iter [168/45] Loss: 0.8689\n",
      "Epoch [6/10], Iter [169/45] Loss: 0.6166\n",
      "Epoch [6/10], Iter [170/45] Loss: 0.7197\n",
      "Epoch [6/10], Iter [171/45] Loss: 0.8862\n",
      "Epoch [6/10], Iter [172/45] Loss: 0.8703\n",
      "Epoch [6/10], Iter [173/45] Loss: 0.5337\n",
      "Epoch [6/10], Iter [174/45] Loss: 1.0385\n",
      "Epoch [6/10], Iter [175/45] Loss: 0.8524\n",
      "Epoch [6/10], Iter [176/45] Loss: 1.0273\n",
      "Epoch [6/10], Iter [177/45] Loss: 0.7091\n",
      "Epoch [6/10], Iter [178/45] Loss: 0.6362\n",
      "Epoch [6/10], Iter [179/45] Loss: 0.8848\n",
      "Epoch [6/10], Iter [180/45] Loss: 0.9932\n",
      "6\n",
      "6\n",
      "Epoch [7/10], Iter [1/45] Loss: 0.7462\n",
      "Epoch [7/10], Iter [2/45] Loss: 0.5276\n",
      "Epoch [7/10], Iter [3/45] Loss: 1.1911\n",
      "Epoch [7/10], Iter [4/45] Loss: 0.7239\n",
      "Epoch [7/10], Iter [5/45] Loss: 0.8799\n",
      "Epoch [7/10], Iter [6/45] Loss: 1.3636\n",
      "Epoch [7/10], Iter [7/45] Loss: 0.8283\n",
      "Epoch [7/10], Iter [8/45] Loss: 1.2545\n",
      "Epoch [7/10], Iter [9/45] Loss: 1.1477\n",
      "Epoch [7/10], Iter [10/45] Loss: 1.2113\n",
      "Epoch [7/10], Iter [11/45] Loss: 0.7083\n",
      "Epoch [7/10], Iter [12/45] Loss: 0.8492\n",
      "Epoch [7/10], Iter [13/45] Loss: 0.6706\n",
      "Epoch [7/10], Iter [14/45] Loss: 0.6843\n",
      "Epoch [7/10], Iter [15/45] Loss: 1.0366\n",
      "Epoch [7/10], Iter [16/45] Loss: 1.1462\n",
      "Epoch [7/10], Iter [17/45] Loss: 0.9271\n",
      "Epoch [7/10], Iter [18/45] Loss: 0.9211\n",
      "Epoch [7/10], Iter [19/45] Loss: 0.9929\n",
      "Epoch [7/10], Iter [20/45] Loss: 0.8389\n",
      "Epoch [7/10], Iter [21/45] Loss: 1.1718\n",
      "Epoch [7/10], Iter [22/45] Loss: 1.0275\n",
      "Epoch [7/10], Iter [23/45] Loss: 1.5356\n",
      "Epoch [7/10], Iter [24/45] Loss: 1.1125\n",
      "Epoch [7/10], Iter [25/45] Loss: 1.0580\n",
      "Epoch [7/10], Iter [26/45] Loss: 1.1296\n",
      "Epoch [7/10], Iter [27/45] Loss: 0.9566\n",
      "Epoch [7/10], Iter [28/45] Loss: 1.2136\n",
      "Epoch [7/10], Iter [29/45] Loss: 0.8202\n",
      "Epoch [7/10], Iter [30/45] Loss: 0.9287\n",
      "Epoch [7/10], Iter [31/45] Loss: 0.6468\n",
      "Epoch [7/10], Iter [32/45] Loss: 0.9277\n",
      "Epoch [7/10], Iter [33/45] Loss: 1.4344\n",
      "Epoch [7/10], Iter [34/45] Loss: 1.5248\n",
      "Epoch [7/10], Iter [35/45] Loss: 1.1728\n",
      "Epoch [7/10], Iter [36/45] Loss: 1.4109\n",
      "Epoch [7/10], Iter [37/45] Loss: 1.0866\n",
      "Epoch [7/10], Iter [38/45] Loss: 1.4850\n",
      "Epoch [7/10], Iter [39/45] Loss: 1.2027\n",
      "Epoch [7/10], Iter [40/45] Loss: 0.9803\n",
      "Epoch [7/10], Iter [41/45] Loss: 1.0002\n",
      "Epoch [7/10], Iter [42/45] Loss: 1.0302\n",
      "Epoch [7/10], Iter [43/45] Loss: 0.7317\n",
      "Epoch [7/10], Iter [44/45] Loss: 0.9862\n",
      "Epoch [7/10], Iter [45/45] Loss: 1.0349\n",
      "Epoch [7/10], Iter [46/45] Loss: 0.9668\n",
      "Epoch [7/10], Iter [47/45] Loss: 1.0354\n",
      "Epoch [7/10], Iter [48/45] Loss: 0.6717\n",
      "Epoch [7/10], Iter [49/45] Loss: 1.0708\n",
      "Epoch [7/10], Iter [50/45] Loss: 0.9399\n",
      "Epoch [7/10], Iter [51/45] Loss: 1.0120\n",
      "Epoch [7/10], Iter [52/45] Loss: 1.3876\n",
      "Epoch [7/10], Iter [53/45] Loss: 0.6595\n",
      "Epoch [7/10], Iter [54/45] Loss: 1.0814\n",
      "Epoch [7/10], Iter [55/45] Loss: 1.3730\n",
      "Epoch [7/10], Iter [56/45] Loss: 1.0729\n",
      "Epoch [7/10], Iter [57/45] Loss: 0.7338\n",
      "Epoch [7/10], Iter [58/45] Loss: 1.2895\n",
      "Epoch [7/10], Iter [59/45] Loss: 0.6703\n",
      "Epoch [7/10], Iter [60/45] Loss: 0.8966\n",
      "Epoch [7/10], Iter [61/45] Loss: 1.0937\n",
      "Epoch [7/10], Iter [62/45] Loss: 1.0075\n",
      "Epoch [7/10], Iter [63/45] Loss: 0.6653\n",
      "Epoch [7/10], Iter [64/45] Loss: 1.2188\n",
      "Epoch [7/10], Iter [65/45] Loss: 0.9206\n",
      "Epoch [7/10], Iter [66/45] Loss: 1.3960\n",
      "Epoch [7/10], Iter [67/45] Loss: 0.9619\n",
      "Epoch [7/10], Iter [68/45] Loss: 0.8781\n",
      "Epoch [7/10], Iter [69/45] Loss: 0.9142\n",
      "Epoch [7/10], Iter [70/45] Loss: 0.7059\n",
      "Epoch [7/10], Iter [71/45] Loss: 1.1381\n",
      "Epoch [7/10], Iter [72/45] Loss: 1.0436\n",
      "Epoch [7/10], Iter [73/45] Loss: 0.7109\n",
      "Epoch [7/10], Iter [74/45] Loss: 0.9581\n",
      "Epoch [7/10], Iter [75/45] Loss: 1.1382\n",
      "Epoch [7/10], Iter [76/45] Loss: 1.0987\n",
      "Epoch [7/10], Iter [77/45] Loss: 1.5197\n",
      "Epoch [7/10], Iter [78/45] Loss: 0.9703\n",
      "Epoch [7/10], Iter [79/45] Loss: 0.6992\n",
      "Epoch [7/10], Iter [80/45] Loss: 0.9411\n",
      "Epoch [7/10], Iter [81/45] Loss: 1.0918\n",
      "Epoch [7/10], Iter [82/45] Loss: 0.8582\n",
      "Epoch [7/10], Iter [83/45] Loss: 1.3530\n",
      "Epoch [7/10], Iter [84/45] Loss: 0.6775\n",
      "Epoch [7/10], Iter [85/45] Loss: 1.0456\n",
      "Epoch [7/10], Iter [86/45] Loss: 1.0506\n",
      "Epoch [7/10], Iter [87/45] Loss: 0.9314\n",
      "Epoch [7/10], Iter [88/45] Loss: 1.0285\n",
      "Epoch [7/10], Iter [89/45] Loss: 0.9978\n",
      "Epoch [7/10], Iter [90/45] Loss: 0.8419\n",
      "Epoch [7/10], Iter [91/45] Loss: 0.6585\n",
      "Epoch [7/10], Iter [92/45] Loss: 0.9010\n",
      "Epoch [7/10], Iter [93/45] Loss: 0.6099\n",
      "Epoch [7/10], Iter [94/45] Loss: 1.2609\n",
      "Epoch [7/10], Iter [95/45] Loss: 0.7247\n",
      "Epoch [7/10], Iter [96/45] Loss: 0.7718\n",
      "Epoch [7/10], Iter [97/45] Loss: 0.9409\n",
      "Epoch [7/10], Iter [98/45] Loss: 0.6923\n",
      "Epoch [7/10], Iter [99/45] Loss: 1.1647\n",
      "Epoch [7/10], Iter [100/45] Loss: 1.3729\n",
      "Epoch [7/10], Iter [101/45] Loss: 0.6935\n",
      "Epoch [7/10], Iter [102/45] Loss: 0.7860\n",
      "Epoch [7/10], Iter [103/45] Loss: 0.7471\n",
      "Epoch [7/10], Iter [104/45] Loss: 1.1757\n",
      "Epoch [7/10], Iter [105/45] Loss: 0.8733\n",
      "Epoch [7/10], Iter [106/45] Loss: 1.3201\n",
      "Epoch [7/10], Iter [107/45] Loss: 0.7885\n",
      "Epoch [7/10], Iter [108/45] Loss: 0.9876\n",
      "Epoch [7/10], Iter [109/45] Loss: 1.2263\n",
      "Epoch [7/10], Iter [110/45] Loss: 1.2328\n",
      "Epoch [7/10], Iter [111/45] Loss: 0.4911\n",
      "Epoch [7/10], Iter [112/45] Loss: 1.3562\n",
      "Epoch [7/10], Iter [113/45] Loss: 0.7518\n",
      "Epoch [7/10], Iter [114/45] Loss: 1.1180\n",
      "Epoch [7/10], Iter [115/45] Loss: 0.8581\n",
      "Epoch [7/10], Iter [116/45] Loss: 1.0682\n",
      "Epoch [7/10], Iter [117/45] Loss: 0.8113\n",
      "Epoch [7/10], Iter [118/45] Loss: 0.9490\n",
      "Epoch [7/10], Iter [119/45] Loss: 0.9740\n",
      "Epoch [7/10], Iter [120/45] Loss: 0.6679\n",
      "Epoch [7/10], Iter [121/45] Loss: 0.9899\n",
      "Epoch [7/10], Iter [122/45] Loss: 1.0488\n",
      "Epoch [7/10], Iter [123/45] Loss: 1.1587\n",
      "Epoch [7/10], Iter [124/45] Loss: 1.2000\n",
      "Epoch [7/10], Iter [125/45] Loss: 1.1159\n",
      "Epoch [7/10], Iter [126/45] Loss: 1.1340\n",
      "Epoch [7/10], Iter [127/45] Loss: 1.0076\n",
      "Epoch [7/10], Iter [128/45] Loss: 1.1289\n",
      "Epoch [7/10], Iter [129/45] Loss: 0.9142\n",
      "Epoch [7/10], Iter [130/45] Loss: 1.2408\n",
      "Epoch [7/10], Iter [131/45] Loss: 0.9248\n",
      "Epoch [7/10], Iter [132/45] Loss: 1.1030\n",
      "Epoch [7/10], Iter [133/45] Loss: 1.1771\n",
      "Epoch [7/10], Iter [134/45] Loss: 1.1456\n",
      "Epoch [7/10], Iter [135/45] Loss: 0.8141\n",
      "Epoch [7/10], Iter [136/45] Loss: 1.4550\n",
      "Epoch [7/10], Iter [137/45] Loss: 1.1322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Iter [138/45] Loss: 1.0729\n",
      "Epoch [7/10], Iter [139/45] Loss: 1.3958\n",
      "Epoch [7/10], Iter [140/45] Loss: 0.8708\n",
      "Epoch [7/10], Iter [141/45] Loss: 1.3227\n",
      "Epoch [7/10], Iter [142/45] Loss: 1.4788\n",
      "Epoch [7/10], Iter [143/45] Loss: 0.7769\n",
      "Epoch [7/10], Iter [144/45] Loss: 0.5157\n",
      "Epoch [7/10], Iter [145/45] Loss: 0.9912\n",
      "Epoch [7/10], Iter [146/45] Loss: 1.1623\n",
      "Epoch [7/10], Iter [147/45] Loss: 0.9416\n",
      "Epoch [7/10], Iter [148/45] Loss: 0.7874\n",
      "Epoch [7/10], Iter [149/45] Loss: 0.8139\n",
      "Epoch [7/10], Iter [150/45] Loss: 0.8467\n",
      "Epoch [7/10], Iter [151/45] Loss: 1.2400\n",
      "Epoch [7/10], Iter [152/45] Loss: 1.3285\n",
      "Epoch [7/10], Iter [153/45] Loss: 1.4910\n",
      "Epoch [7/10], Iter [154/45] Loss: 0.8328\n",
      "Epoch [7/10], Iter [155/45] Loss: 1.0494\n",
      "Epoch [7/10], Iter [156/45] Loss: 1.0057\n",
      "Epoch [7/10], Iter [157/45] Loss: 0.9464\n",
      "Epoch [7/10], Iter [158/45] Loss: 0.9192\n",
      "Epoch [7/10], Iter [159/45] Loss: 1.1502\n",
      "Epoch [7/10], Iter [160/45] Loss: 1.3589\n",
      "Epoch [7/10], Iter [161/45] Loss: 0.9159\n",
      "Epoch [7/10], Iter [162/45] Loss: 1.1848\n",
      "Epoch [7/10], Iter [163/45] Loss: 0.9862\n",
      "Epoch [7/10], Iter [164/45] Loss: 1.4705\n",
      "Epoch [7/10], Iter [165/45] Loss: 1.2667\n",
      "Epoch [7/10], Iter [166/45] Loss: 0.9552\n",
      "Epoch [7/10], Iter [167/45] Loss: 1.1717\n",
      "Epoch [7/10], Iter [168/45] Loss: 0.8174\n",
      "Epoch [7/10], Iter [169/45] Loss: 0.9223\n",
      "Epoch [7/10], Iter [170/45] Loss: 1.0318\n",
      "Epoch [7/10], Iter [171/45] Loss: 1.0540\n",
      "Epoch [7/10], Iter [172/45] Loss: 1.5064\n",
      "Epoch [7/10], Iter [173/45] Loss: 0.9643\n",
      "Epoch [7/10], Iter [174/45] Loss: 0.9739\n",
      "Epoch [7/10], Iter [175/45] Loss: 0.9669\n",
      "Epoch [7/10], Iter [176/45] Loss: 0.7996\n",
      "Epoch [7/10], Iter [177/45] Loss: 0.9984\n",
      "Epoch [7/10], Iter [178/45] Loss: 0.5565\n",
      "Epoch [7/10], Iter [179/45] Loss: 1.1516\n",
      "Epoch [7/10], Iter [180/45] Loss: 1.4512\n",
      "7\n",
      "7\n",
      "Epoch [8/10], Iter [1/45] Loss: 0.9463\n",
      "Epoch [8/10], Iter [2/45] Loss: 0.5874\n",
      "Epoch [8/10], Iter [3/45] Loss: 1.0446\n",
      "Epoch [8/10], Iter [4/45] Loss: 0.7772\n",
      "Epoch [8/10], Iter [5/45] Loss: 0.9384\n",
      "Epoch [8/10], Iter [6/45] Loss: 0.9926\n",
      "Epoch [8/10], Iter [7/45] Loss: 0.8886\n",
      "Epoch [8/10], Iter [8/45] Loss: 0.9771\n",
      "Epoch [8/10], Iter [9/45] Loss: 1.3380\n",
      "Epoch [8/10], Iter [10/45] Loss: 0.7270\n",
      "Epoch [8/10], Iter [11/45] Loss: 0.9147\n",
      "Epoch [8/10], Iter [12/45] Loss: 1.2349\n",
      "Epoch [8/10], Iter [13/45] Loss: 1.2910\n",
      "Epoch [8/10], Iter [14/45] Loss: 1.1102\n",
      "Epoch [8/10], Iter [15/45] Loss: 1.0701\n",
      "Epoch [8/10], Iter [16/45] Loss: 0.8948\n",
      "Epoch [8/10], Iter [17/45] Loss: 0.8461\n",
      "Epoch [8/10], Iter [18/45] Loss: 0.8709\n",
      "Epoch [8/10], Iter [19/45] Loss: 0.9572\n",
      "Epoch [8/10], Iter [20/45] Loss: 1.2901\n",
      "Epoch [8/10], Iter [21/45] Loss: 0.7075\n",
      "Epoch [8/10], Iter [22/45] Loss: 1.1162\n",
      "Epoch [8/10], Iter [23/45] Loss: 0.9710\n",
      "Epoch [8/10], Iter [24/45] Loss: 1.7000\n",
      "Epoch [8/10], Iter [25/45] Loss: 0.6494\n",
      "Epoch [8/10], Iter [26/45] Loss: 1.0730\n",
      "Epoch [8/10], Iter [27/45] Loss: 0.9629\n",
      "Epoch [8/10], Iter [28/45] Loss: 0.9214\n",
      "Epoch [8/10], Iter [29/45] Loss: 0.8261\n",
      "Epoch [8/10], Iter [30/45] Loss: 0.8081\n",
      "Epoch [8/10], Iter [31/45] Loss: 0.9262\n",
      "Epoch [8/10], Iter [32/45] Loss: 0.8984\n",
      "Epoch [8/10], Iter [33/45] Loss: 0.9006\n",
      "Epoch [8/10], Iter [34/45] Loss: 1.2604\n",
      "Epoch [8/10], Iter [35/45] Loss: 1.0656\n",
      "Epoch [8/10], Iter [36/45] Loss: 1.0035\n",
      "Epoch [8/10], Iter [37/45] Loss: 0.6922\n",
      "Epoch [8/10], Iter [38/45] Loss: 0.8311\n",
      "Epoch [8/10], Iter [39/45] Loss: 0.9806\n",
      "Epoch [8/10], Iter [40/45] Loss: 1.2939\n",
      "Epoch [8/10], Iter [41/45] Loss: 0.9582\n",
      "Epoch [8/10], Iter [42/45] Loss: 1.1216\n",
      "Epoch [8/10], Iter [43/45] Loss: 0.9371\n",
      "Epoch [8/10], Iter [44/45] Loss: 0.7650\n",
      "Epoch [8/10], Iter [45/45] Loss: 0.9529\n",
      "Epoch [8/10], Iter [46/45] Loss: 0.9439\n",
      "Epoch [8/10], Iter [47/45] Loss: 1.1887\n",
      "Epoch [8/10], Iter [48/45] Loss: 0.7971\n",
      "Epoch [8/10], Iter [49/45] Loss: 1.2149\n",
      "Epoch [8/10], Iter [50/45] Loss: 1.2248\n",
      "Epoch [8/10], Iter [51/45] Loss: 0.8327\n",
      "Epoch [8/10], Iter [52/45] Loss: 0.7525\n",
      "Epoch [8/10], Iter [53/45] Loss: 1.2975\n",
      "Epoch [8/10], Iter [54/45] Loss: 0.7574\n",
      "Epoch [8/10], Iter [55/45] Loss: 0.5628\n",
      "Epoch [8/10], Iter [56/45] Loss: 1.2033\n",
      "Epoch [8/10], Iter [57/45] Loss: 1.2139\n",
      "Epoch [8/10], Iter [58/45] Loss: 0.8522\n",
      "Epoch [8/10], Iter [59/45] Loss: 1.4845\n",
      "Epoch [8/10], Iter [60/45] Loss: 1.0781\n",
      "Epoch [8/10], Iter [61/45] Loss: 0.9477\n",
      "Epoch [8/10], Iter [62/45] Loss: 0.9649\n",
      "Epoch [8/10], Iter [63/45] Loss: 1.2382\n",
      "Epoch [8/10], Iter [64/45] Loss: 0.8449\n",
      "Epoch [8/10], Iter [65/45] Loss: 1.4584\n",
      "Epoch [8/10], Iter [66/45] Loss: 1.1101\n",
      "Epoch [8/10], Iter [67/45] Loss: 0.6299\n",
      "Epoch [8/10], Iter [68/45] Loss: 1.2093\n",
      "Epoch [8/10], Iter [69/45] Loss: 0.8177\n",
      "Epoch [8/10], Iter [70/45] Loss: 1.0356\n",
      "Epoch [8/10], Iter [71/45] Loss: 0.8031\n",
      "Epoch [8/10], Iter [72/45] Loss: 1.1271\n",
      "Epoch [8/10], Iter [73/45] Loss: 1.0948\n",
      "Epoch [8/10], Iter [74/45] Loss: 0.9411\n",
      "Epoch [8/10], Iter [75/45] Loss: 1.2194\n",
      "Epoch [8/10], Iter [76/45] Loss: 1.1421\n",
      "Epoch [8/10], Iter [77/45] Loss: 0.7319\n",
      "Epoch [8/10], Iter [78/45] Loss: 0.8359\n",
      "Epoch [8/10], Iter [79/45] Loss: 0.9477\n",
      "Epoch [8/10], Iter [80/45] Loss: 1.0728\n",
      "Epoch [8/10], Iter [81/45] Loss: 0.9688\n",
      "Epoch [8/10], Iter [82/45] Loss: 1.0567\n",
      "Epoch [8/10], Iter [83/45] Loss: 1.1074\n",
      "Epoch [8/10], Iter [84/45] Loss: 0.9400\n",
      "Epoch [8/10], Iter [85/45] Loss: 1.2930\n",
      "Epoch [8/10], Iter [86/45] Loss: 1.0106\n",
      "Epoch [8/10], Iter [87/45] Loss: 0.8003\n",
      "Epoch [8/10], Iter [88/45] Loss: 1.0301\n",
      "Epoch [8/10], Iter [89/45] Loss: 1.3519\n",
      "Epoch [8/10], Iter [90/45] Loss: 0.8590\n",
      "Epoch [8/10], Iter [91/45] Loss: 1.0555\n",
      "Epoch [8/10], Iter [92/45] Loss: 0.6605\n",
      "Epoch [8/10], Iter [93/45] Loss: 0.8430\n",
      "Epoch [8/10], Iter [94/45] Loss: 0.8676\n",
      "Epoch [8/10], Iter [95/45] Loss: 1.0716\n",
      "Epoch [8/10], Iter [96/45] Loss: 0.9689\n",
      "Epoch [8/10], Iter [97/45] Loss: 0.7217\n",
      "Epoch [8/10], Iter [98/45] Loss: 1.4178\n",
      "Epoch [8/10], Iter [99/45] Loss: 1.2834\n",
      "Epoch [8/10], Iter [100/45] Loss: 1.0883\n",
      "Epoch [8/10], Iter [101/45] Loss: 1.0953\n",
      "Epoch [8/10], Iter [102/45] Loss: 1.1885\n",
      "Epoch [8/10], Iter [103/45] Loss: 0.9525\n",
      "Epoch [8/10], Iter [104/45] Loss: 0.7346\n",
      "Epoch [8/10], Iter [105/45] Loss: 0.6851\n",
      "Epoch [8/10], Iter [106/45] Loss: 0.7376\n",
      "Epoch [8/10], Iter [107/45] Loss: 1.0750\n",
      "Epoch [8/10], Iter [108/45] Loss: 0.6708\n",
      "Epoch [8/10], Iter [109/45] Loss: 1.0376\n",
      "Epoch [8/10], Iter [110/45] Loss: 0.9063\n",
      "Epoch [8/10], Iter [111/45] Loss: 0.6467\n",
      "Epoch [8/10], Iter [112/45] Loss: 0.9892\n",
      "Epoch [8/10], Iter [113/45] Loss: 0.7336\n",
      "Epoch [8/10], Iter [114/45] Loss: 1.2444\n",
      "Epoch [8/10], Iter [115/45] Loss: 0.8948\n",
      "Epoch [8/10], Iter [116/45] Loss: 1.3344\n",
      "Epoch [8/10], Iter [117/45] Loss: 1.0444\n",
      "Epoch [8/10], Iter [118/45] Loss: 1.2189\n",
      "Epoch [8/10], Iter [119/45] Loss: 1.0453\n",
      "Epoch [8/10], Iter [120/45] Loss: 1.2618\n",
      "Epoch [8/10], Iter [121/45] Loss: 0.7665\n",
      "Epoch [8/10], Iter [122/45] Loss: 0.8175\n",
      "Epoch [8/10], Iter [123/45] Loss: 0.7344\n",
      "Epoch [8/10], Iter [124/45] Loss: 0.7523\n",
      "Epoch [8/10], Iter [125/45] Loss: 1.1411\n",
      "Epoch [8/10], Iter [126/45] Loss: 0.8807\n",
      "Epoch [8/10], Iter [127/45] Loss: 0.8316\n",
      "Epoch [8/10], Iter [128/45] Loss: 1.0916\n",
      "Epoch [8/10], Iter [129/45] Loss: 0.6349\n",
      "Epoch [8/10], Iter [130/45] Loss: 0.7908\n",
      "Epoch [8/10], Iter [131/45] Loss: 0.8357\n",
      "Epoch [8/10], Iter [132/45] Loss: 0.6813\n",
      "Epoch [8/10], Iter [133/45] Loss: 0.8088\n",
      "Epoch [8/10], Iter [134/45] Loss: 0.7487\n",
      "Epoch [8/10], Iter [135/45] Loss: 1.4414\n",
      "Epoch [8/10], Iter [136/45] Loss: 1.0512\n",
      "Epoch [8/10], Iter [137/45] Loss: 1.2756\n",
      "Epoch [8/10], Iter [138/45] Loss: 0.6377\n",
      "Epoch [8/10], Iter [139/45] Loss: 1.1417\n",
      "Epoch [8/10], Iter [140/45] Loss: 0.8606\n",
      "Epoch [8/10], Iter [141/45] Loss: 1.2344\n",
      "Epoch [8/10], Iter [142/45] Loss: 1.1522\n",
      "Epoch [8/10], Iter [143/45] Loss: 1.2254\n",
      "Epoch [8/10], Iter [144/45] Loss: 1.1733\n",
      "Epoch [8/10], Iter [145/45] Loss: 1.1165\n",
      "Epoch [8/10], Iter [146/45] Loss: 0.8320\n",
      "Epoch [8/10], Iter [147/45] Loss: 0.7417\n",
      "Epoch [8/10], Iter [148/45] Loss: 1.3514\n",
      "Epoch [8/10], Iter [149/45] Loss: 0.8604\n",
      "Epoch [8/10], Iter [150/45] Loss: 1.1485\n",
      "Epoch [8/10], Iter [151/45] Loss: 1.1259\n",
      "Epoch [8/10], Iter [152/45] Loss: 1.1901\n",
      "Epoch [8/10], Iter [153/45] Loss: 0.9276\n",
      "Epoch [8/10], Iter [154/45] Loss: 0.9876\n",
      "Epoch [8/10], Iter [155/45] Loss: 0.8886\n",
      "Epoch [8/10], Iter [156/45] Loss: 1.4750\n",
      "Epoch [8/10], Iter [157/45] Loss: 0.9903\n",
      "Epoch [8/10], Iter [158/45] Loss: 1.3126\n",
      "Epoch [8/10], Iter [159/45] Loss: 0.9406\n",
      "Epoch [8/10], Iter [160/45] Loss: 0.7845\n",
      "Epoch [8/10], Iter [161/45] Loss: 1.2438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Iter [162/45] Loss: 1.4463\n",
      "Epoch [8/10], Iter [163/45] Loss: 1.0407\n",
      "Epoch [8/10], Iter [164/45] Loss: 0.7374\n",
      "Epoch [8/10], Iter [165/45] Loss: 0.9917\n",
      "Epoch [8/10], Iter [166/45] Loss: 0.6490\n",
      "Epoch [8/10], Iter [167/45] Loss: 1.0990\n",
      "Epoch [8/10], Iter [168/45] Loss: 1.4965\n",
      "Epoch [8/10], Iter [169/45] Loss: 0.8243\n",
      "Epoch [8/10], Iter [170/45] Loss: 1.1199\n",
      "Epoch [8/10], Iter [171/45] Loss: 1.2633\n",
      "Epoch [8/10], Iter [172/45] Loss: 1.4857\n",
      "Epoch [8/10], Iter [173/45] Loss: 1.2325\n",
      "Epoch [8/10], Iter [174/45] Loss: 0.8197\n",
      "Epoch [8/10], Iter [175/45] Loss: 1.0929\n",
      "Epoch [8/10], Iter [176/45] Loss: 1.0537\n",
      "Epoch [8/10], Iter [177/45] Loss: 0.8259\n",
      "Epoch [8/10], Iter [178/45] Loss: 1.0822\n",
      "Epoch [8/10], Iter [179/45] Loss: 1.7240\n",
      "Epoch [8/10], Iter [180/45] Loss: 0.6358\n",
      "8\n",
      "8\n",
      "Epoch [9/10], Iter [1/45] Loss: 1.1536\n",
      "Epoch [9/10], Iter [2/45] Loss: 1.1491\n",
      "Epoch [9/10], Iter [3/45] Loss: 0.8138\n",
      "Epoch [9/10], Iter [4/45] Loss: 0.7503\n",
      "Epoch [9/10], Iter [5/45] Loss: 0.6483\n",
      "Epoch [9/10], Iter [6/45] Loss: 0.9338\n",
      "Epoch [9/10], Iter [7/45] Loss: 0.9163\n",
      "Epoch [9/10], Iter [8/45] Loss: 1.0303\n",
      "Epoch [9/10], Iter [9/45] Loss: 0.5355\n",
      "Epoch [9/10], Iter [10/45] Loss: 0.8537\n",
      "Epoch [9/10], Iter [11/45] Loss: 0.9621\n",
      "Epoch [9/10], Iter [12/45] Loss: 0.8139\n",
      "Epoch [9/10], Iter [13/45] Loss: 1.5069\n",
      "Epoch [9/10], Iter [14/45] Loss: 1.4550\n",
      "Epoch [9/10], Iter [15/45] Loss: 0.7701\n",
      "Epoch [9/10], Iter [16/45] Loss: 1.2372\n",
      "Epoch [9/10], Iter [17/45] Loss: 1.0160\n",
      "Epoch [9/10], Iter [18/45] Loss: 0.9271\n",
      "Epoch [9/10], Iter [19/45] Loss: 1.3397\n",
      "Epoch [9/10], Iter [20/45] Loss: 1.3571\n",
      "Epoch [9/10], Iter [21/45] Loss: 0.9343\n",
      "Epoch [9/10], Iter [22/45] Loss: 0.9132\n",
      "Epoch [9/10], Iter [23/45] Loss: 1.0990\n",
      "Epoch [9/10], Iter [24/45] Loss: 1.4314\n",
      "Epoch [9/10], Iter [25/45] Loss: 1.4812\n",
      "Epoch [9/10], Iter [26/45] Loss: 1.0424\n",
      "Epoch [9/10], Iter [27/45] Loss: 0.8193\n",
      "Epoch [9/10], Iter [28/45] Loss: 1.3996\n",
      "Epoch [9/10], Iter [29/45] Loss: 0.8418\n",
      "Epoch [9/10], Iter [30/45] Loss: 1.1451\n",
      "Epoch [9/10], Iter [31/45] Loss: 1.2545\n",
      "Epoch [9/10], Iter [32/45] Loss: 1.0252\n",
      "Epoch [9/10], Iter [33/45] Loss: 1.2091\n",
      "Epoch [9/10], Iter [34/45] Loss: 1.0421\n",
      "Epoch [9/10], Iter [35/45] Loss: 0.9727\n",
      "Epoch [9/10], Iter [36/45] Loss: 1.0593\n",
      "Epoch [9/10], Iter [37/45] Loss: 0.7932\n",
      "Epoch [9/10], Iter [38/45] Loss: 1.1385\n",
      "Epoch [9/10], Iter [39/45] Loss: 1.1772\n",
      "Epoch [9/10], Iter [40/45] Loss: 1.0455\n",
      "Epoch [9/10], Iter [41/45] Loss: 1.4342\n",
      "Epoch [9/10], Iter [42/45] Loss: 0.7159\n",
      "Epoch [9/10], Iter [43/45] Loss: 0.7379\n",
      "Epoch [9/10], Iter [44/45] Loss: 1.6042\n",
      "Epoch [9/10], Iter [45/45] Loss: 0.6282\n",
      "Epoch [9/10], Iter [46/45] Loss: 0.9731\n",
      "Epoch [9/10], Iter [47/45] Loss: 1.2854\n",
      "Epoch [9/10], Iter [48/45] Loss: 1.0597\n",
      "Epoch [9/10], Iter [49/45] Loss: 1.3442\n",
      "Epoch [9/10], Iter [50/45] Loss: 0.7558\n",
      "Epoch [9/10], Iter [51/45] Loss: 0.8683\n",
      "Epoch [9/10], Iter [52/45] Loss: 1.0988\n",
      "Epoch [9/10], Iter [53/45] Loss: 0.8636\n",
      "Epoch [9/10], Iter [54/45] Loss: 1.4148\n",
      "Epoch [9/10], Iter [55/45] Loss: 1.0676\n",
      "Epoch [9/10], Iter [56/45] Loss: 1.1472\n",
      "Epoch [9/10], Iter [57/45] Loss: 0.7969\n",
      "Epoch [9/10], Iter [58/45] Loss: 1.3284\n",
      "Epoch [9/10], Iter [59/45] Loss: 0.8632\n",
      "Epoch [9/10], Iter [60/45] Loss: 0.9273\n",
      "Epoch [9/10], Iter [61/45] Loss: 0.9672\n",
      "Epoch [9/10], Iter [62/45] Loss: 0.7985\n",
      "Epoch [9/10], Iter [63/45] Loss: 1.0341\n",
      "Epoch [9/10], Iter [64/45] Loss: 0.8223\n",
      "Epoch [9/10], Iter [65/45] Loss: 1.2644\n",
      "Epoch [9/10], Iter [66/45] Loss: 1.1191\n",
      "Epoch [9/10], Iter [67/45] Loss: 0.9764\n",
      "Epoch [9/10], Iter [68/45] Loss: 0.9821\n",
      "Epoch [9/10], Iter [69/45] Loss: 0.7227\n",
      "Epoch [9/10], Iter [70/45] Loss: 0.7479\n",
      "Epoch [9/10], Iter [71/45] Loss: 0.6766\n",
      "Epoch [9/10], Iter [72/45] Loss: 1.3517\n",
      "Epoch [9/10], Iter [73/45] Loss: 0.6494\n",
      "Epoch [9/10], Iter [74/45] Loss: 0.7571\n",
      "Epoch [9/10], Iter [75/45] Loss: 0.7111\n",
      "Epoch [9/10], Iter [76/45] Loss: 1.2711\n",
      "Epoch [9/10], Iter [77/45] Loss: 0.8472\n",
      "Epoch [9/10], Iter [78/45] Loss: 0.5266\n",
      "Epoch [9/10], Iter [79/45] Loss: 0.9002\n",
      "Epoch [9/10], Iter [80/45] Loss: 1.0513\n",
      "Epoch [9/10], Iter [81/45] Loss: 0.9838\n",
      "Epoch [9/10], Iter [82/45] Loss: 0.7191\n",
      "Epoch [9/10], Iter [83/45] Loss: 0.6436\n",
      "Epoch [9/10], Iter [84/45] Loss: 0.9587\n",
      "Epoch [9/10], Iter [85/45] Loss: 0.7008\n",
      "Epoch [9/10], Iter [86/45] Loss: 1.2077\n",
      "Epoch [9/10], Iter [87/45] Loss: 1.1966\n",
      "Epoch [9/10], Iter [88/45] Loss: 1.4258\n",
      "Epoch [9/10], Iter [89/45] Loss: 1.2184\n",
      "Epoch [9/10], Iter [90/45] Loss: 1.1202\n",
      "Epoch [9/10], Iter [91/45] Loss: 0.5430\n",
      "Epoch [9/10], Iter [92/45] Loss: 1.1554\n",
      "Epoch [9/10], Iter [93/45] Loss: 1.1883\n",
      "Epoch [9/10], Iter [94/45] Loss: 0.7734\n",
      "Epoch [9/10], Iter [95/45] Loss: 0.8648\n",
      "Epoch [9/10], Iter [96/45] Loss: 0.8720\n",
      "Epoch [9/10], Iter [97/45] Loss: 1.2833\n",
      "Epoch [9/10], Iter [98/45] Loss: 1.0190\n",
      "Epoch [9/10], Iter [99/45] Loss: 1.1641\n",
      "Epoch [9/10], Iter [100/45] Loss: 0.6374\n",
      "Epoch [9/10], Iter [101/45] Loss: 0.8199\n",
      "Epoch [9/10], Iter [102/45] Loss: 0.9280\n",
      "Epoch [9/10], Iter [103/45] Loss: 0.9277\n",
      "Epoch [9/10], Iter [104/45] Loss: 1.2825\n",
      "Epoch [9/10], Iter [105/45] Loss: 0.6061\n",
      "Epoch [9/10], Iter [106/45] Loss: 0.6503\n",
      "Epoch [9/10], Iter [107/45] Loss: 1.2402\n",
      "Epoch [9/10], Iter [108/45] Loss: 0.9060\n",
      "Epoch [9/10], Iter [109/45] Loss: 1.0021\n",
      "Epoch [9/10], Iter [110/45] Loss: 1.1419\n",
      "Epoch [9/10], Iter [111/45] Loss: 0.4777\n",
      "Epoch [9/10], Iter [112/45] Loss: 1.3509\n",
      "Epoch [9/10], Iter [113/45] Loss: 1.1198\n",
      "Epoch [9/10], Iter [114/45] Loss: 0.9791\n",
      "Epoch [9/10], Iter [115/45] Loss: 1.1666\n",
      "Epoch [9/10], Iter [116/45] Loss: 0.9337\n",
      "Epoch [9/10], Iter [117/45] Loss: 1.3598\n",
      "Epoch [9/10], Iter [118/45] Loss: 0.9747\n",
      "Epoch [9/10], Iter [119/45] Loss: 0.6525\n",
      "Epoch [9/10], Iter [120/45] Loss: 0.9683\n",
      "Epoch [9/10], Iter [121/45] Loss: 1.1413\n",
      "Epoch [9/10], Iter [122/45] Loss: 0.7280\n",
      "Epoch [9/10], Iter [123/45] Loss: 0.6054\n",
      "Epoch [9/10], Iter [124/45] Loss: 0.6268\n",
      "Epoch [9/10], Iter [125/45] Loss: 1.2735\n",
      "Epoch [9/10], Iter [126/45] Loss: 1.0257\n",
      "Epoch [9/10], Iter [127/45] Loss: 1.1636\n",
      "Epoch [9/10], Iter [128/45] Loss: 1.0280\n",
      "Epoch [9/10], Iter [129/45] Loss: 0.8470\n",
      "Epoch [9/10], Iter [130/45] Loss: 1.0788\n",
      "Epoch [9/10], Iter [131/45] Loss: 1.2238\n",
      "Epoch [9/10], Iter [132/45] Loss: 1.2916\n",
      "Epoch [9/10], Iter [133/45] Loss: 0.9310\n",
      "Epoch [9/10], Iter [134/45] Loss: 0.9528\n",
      "Epoch [9/10], Iter [135/45] Loss: 0.9900\n",
      "Epoch [9/10], Iter [136/45] Loss: 0.9743\n",
      "Epoch [9/10], Iter [137/45] Loss: 0.6654\n",
      "Epoch [9/10], Iter [138/45] Loss: 1.4106\n",
      "Epoch [9/10], Iter [139/45] Loss: 0.8082\n",
      "Epoch [9/10], Iter [140/45] Loss: 0.8240\n",
      "Epoch [9/10], Iter [141/45] Loss: 1.1529\n",
      "Epoch [9/10], Iter [142/45] Loss: 0.9622\n",
      "Epoch [9/10], Iter [143/45] Loss: 1.1603\n",
      "Epoch [9/10], Iter [144/45] Loss: 0.6710\n",
      "Epoch [9/10], Iter [145/45] Loss: 0.8964\n",
      "Epoch [9/10], Iter [146/45] Loss: 0.9005\n",
      "Epoch [9/10], Iter [147/45] Loss: 0.7353\n",
      "Epoch [9/10], Iter [148/45] Loss: 0.7419\n",
      "Epoch [9/10], Iter [149/45] Loss: 1.1477\n",
      "Epoch [9/10], Iter [150/45] Loss: 0.5482\n",
      "Epoch [9/10], Iter [151/45] Loss: 0.7124\n",
      "Epoch [9/10], Iter [152/45] Loss: 1.2073\n",
      "Epoch [9/10], Iter [153/45] Loss: 0.8198\n",
      "Epoch [9/10], Iter [154/45] Loss: 0.6103\n",
      "Epoch [9/10], Iter [155/45] Loss: 0.7517\n",
      "Epoch [9/10], Iter [156/45] Loss: 0.8917\n",
      "Epoch [9/10], Iter [157/45] Loss: 0.9330\n",
      "Epoch [9/10], Iter [158/45] Loss: 0.6731\n",
      "Epoch [9/10], Iter [159/45] Loss: 0.8740\n",
      "Epoch [9/10], Iter [160/45] Loss: 1.1169\n",
      "Epoch [9/10], Iter [161/45] Loss: 1.3082\n",
      "Epoch [9/10], Iter [162/45] Loss: 0.9094\n",
      "Epoch [9/10], Iter [163/45] Loss: 0.8436\n",
      "Epoch [9/10], Iter [164/45] Loss: 1.1331\n",
      "Epoch [9/10], Iter [165/45] Loss: 0.5351\n",
      "Epoch [9/10], Iter [166/45] Loss: 0.6834\n",
      "Epoch [9/10], Iter [167/45] Loss: 0.6900\n",
      "Epoch [9/10], Iter [168/45] Loss: 1.0461\n",
      "Epoch [9/10], Iter [169/45] Loss: 1.1351\n",
      "Epoch [9/10], Iter [170/45] Loss: 0.7967\n",
      "Epoch [9/10], Iter [171/45] Loss: 0.7774\n",
      "Epoch [9/10], Iter [172/45] Loss: 1.1551\n",
      "Epoch [9/10], Iter [173/45] Loss: 0.8636\n",
      "Epoch [9/10], Iter [174/45] Loss: 1.0925\n",
      "Epoch [9/10], Iter [175/45] Loss: 1.1212\n",
      "Epoch [9/10], Iter [176/45] Loss: 1.0346\n",
      "Epoch [9/10], Iter [177/45] Loss: 1.3859\n",
      "Epoch [9/10], Iter [178/45] Loss: 0.7536\n",
      "Epoch [9/10], Iter [179/45] Loss: 0.7007\n",
      "Epoch [9/10], Iter [180/45] Loss: 0.8620\n",
      "9\n",
      "9\n",
      "Epoch [10/10], Iter [1/45] Loss: 1.1016\n",
      "Epoch [10/10], Iter [2/45] Loss: 0.5666\n",
      "Epoch [10/10], Iter [3/45] Loss: 0.9433\n",
      "Epoch [10/10], Iter [4/45] Loss: 1.1544\n",
      "Epoch [10/10], Iter [5/45] Loss: 0.9668\n",
      "Epoch [10/10], Iter [6/45] Loss: 1.0102\n",
      "Epoch [10/10], Iter [7/45] Loss: 0.9241\n",
      "Epoch [10/10], Iter [8/45] Loss: 0.7751\n",
      "Epoch [10/10], Iter [9/45] Loss: 1.4611\n",
      "Epoch [10/10], Iter [10/45] Loss: 1.0990\n",
      "Epoch [10/10], Iter [11/45] Loss: 0.9780\n",
      "Epoch [10/10], Iter [12/45] Loss: 0.8171\n",
      "Epoch [10/10], Iter [13/45] Loss: 1.0797\n",
      "Epoch [10/10], Iter [14/45] Loss: 0.9340\n",
      "Epoch [10/10], Iter [15/45] Loss: 1.3289\n",
      "Epoch [10/10], Iter [16/45] Loss: 1.1447\n",
      "Epoch [10/10], Iter [17/45] Loss: 0.6933\n",
      "Epoch [10/10], Iter [18/45] Loss: 1.1254\n",
      "Epoch [10/10], Iter [19/45] Loss: 0.9465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Iter [20/45] Loss: 1.0273\n",
      "Epoch [10/10], Iter [21/45] Loss: 0.8795\n",
      "Epoch [10/10], Iter [22/45] Loss: 0.7327\n",
      "Epoch [10/10], Iter [23/45] Loss: 1.2280\n",
      "Epoch [10/10], Iter [24/45] Loss: 0.8527\n",
      "Epoch [10/10], Iter [25/45] Loss: 0.6092\n",
      "Epoch [10/10], Iter [26/45] Loss: 0.7886\n",
      "Epoch [10/10], Iter [27/45] Loss: 1.0343\n",
      "Epoch [10/10], Iter [28/45] Loss: 1.1258\n",
      "Epoch [10/10], Iter [29/45] Loss: 0.8313\n",
      "Epoch [10/10], Iter [30/45] Loss: 1.1518\n",
      "Epoch [10/10], Iter [31/45] Loss: 0.8172\n",
      "Epoch [10/10], Iter [32/45] Loss: 0.8269\n",
      "Epoch [10/10], Iter [33/45] Loss: 0.8386\n",
      "Epoch [10/10], Iter [34/45] Loss: 1.2737\n",
      "Epoch [10/10], Iter [35/45] Loss: 1.1599\n",
      "Epoch [10/10], Iter [36/45] Loss: 1.0866\n",
      "Epoch [10/10], Iter [37/45] Loss: 0.8861\n",
      "Epoch [10/10], Iter [38/45] Loss: 0.8855\n",
      "Epoch [10/10], Iter [39/45] Loss: 0.6434\n",
      "Epoch [10/10], Iter [40/45] Loss: 1.1347\n",
      "Epoch [10/10], Iter [41/45] Loss: 0.5028\n",
      "Epoch [10/10], Iter [42/45] Loss: 1.1290\n",
      "Epoch [10/10], Iter [43/45] Loss: 1.0575\n",
      "Epoch [10/10], Iter [44/45] Loss: 0.8263\n",
      "Epoch [10/10], Iter [45/45] Loss: 0.9824\n",
      "Epoch [10/10], Iter [46/45] Loss: 0.5608\n",
      "Epoch [10/10], Iter [47/45] Loss: 1.1772\n",
      "Epoch [10/10], Iter [48/45] Loss: 1.2501\n",
      "Epoch [10/10], Iter [49/45] Loss: 1.0831\n",
      "Epoch [10/10], Iter [50/45] Loss: 0.8527\n",
      "Epoch [10/10], Iter [51/45] Loss: 0.9125\n",
      "Epoch [10/10], Iter [52/45] Loss: 1.1228\n",
      "Epoch [10/10], Iter [53/45] Loss: 0.9348\n",
      "Epoch [10/10], Iter [54/45] Loss: 0.9661\n",
      "Epoch [10/10], Iter [55/45] Loss: 0.9456\n",
      "Epoch [10/10], Iter [56/45] Loss: 1.2164\n",
      "Epoch [10/10], Iter [57/45] Loss: 0.8566\n",
      "Epoch [10/10], Iter [58/45] Loss: 0.7435\n",
      "Epoch [10/10], Iter [59/45] Loss: 0.7499\n",
      "Epoch [10/10], Iter [60/45] Loss: 0.9363\n",
      "Epoch [10/10], Iter [61/45] Loss: 1.2302\n",
      "Epoch [10/10], Iter [62/45] Loss: 0.6390\n",
      "Epoch [10/10], Iter [63/45] Loss: 1.0390\n",
      "Epoch [10/10], Iter [64/45] Loss: 1.3022\n",
      "Epoch [10/10], Iter [65/45] Loss: 1.0194\n",
      "Epoch [10/10], Iter [66/45] Loss: 0.9520\n",
      "Epoch [10/10], Iter [67/45] Loss: 1.2163\n",
      "Epoch [10/10], Iter [68/45] Loss: 0.8086\n",
      "Epoch [10/10], Iter [69/45] Loss: 1.0090\n",
      "Epoch [10/10], Iter [70/45] Loss: 1.0113\n",
      "Epoch [10/10], Iter [71/45] Loss: 0.9751\n",
      "Epoch [10/10], Iter [72/45] Loss: 1.0950\n",
      "Epoch [10/10], Iter [73/45] Loss: 0.7520\n",
      "Epoch [10/10], Iter [74/45] Loss: 1.7090\n",
      "Epoch [10/10], Iter [75/45] Loss: 0.6743\n",
      "Epoch [10/10], Iter [76/45] Loss: 0.9307\n",
      "Epoch [10/10], Iter [77/45] Loss: 0.9550\n",
      "Epoch [10/10], Iter [78/45] Loss: 0.8854\n",
      "Epoch [10/10], Iter [79/45] Loss: 0.7233\n",
      "Epoch [10/10], Iter [80/45] Loss: 0.7563\n",
      "Epoch [10/10], Iter [81/45] Loss: 1.6937\n",
      "Epoch [10/10], Iter [82/45] Loss: 0.7593\n",
      "Epoch [10/10], Iter [83/45] Loss: 1.2407\n",
      "Epoch [10/10], Iter [84/45] Loss: 1.4507\n",
      "Epoch [10/10], Iter [85/45] Loss: 1.3529\n",
      "Epoch [10/10], Iter [86/45] Loss: 1.1792\n",
      "Epoch [10/10], Iter [87/45] Loss: 0.9795\n",
      "Epoch [10/10], Iter [88/45] Loss: 0.8913\n",
      "Epoch [10/10], Iter [89/45] Loss: 1.0593\n",
      "Epoch [10/10], Iter [90/45] Loss: 0.7577\n",
      "Epoch [10/10], Iter [91/45] Loss: 1.3677\n",
      "Epoch [10/10], Iter [92/45] Loss: 0.7692\n",
      "Epoch [10/10], Iter [93/45] Loss: 1.1286\n",
      "Epoch [10/10], Iter [94/45] Loss: 0.6291\n",
      "Epoch [10/10], Iter [95/45] Loss: 0.8753\n",
      "Epoch [10/10], Iter [96/45] Loss: 0.8827\n",
      "Epoch [10/10], Iter [97/45] Loss: 1.1085\n",
      "Epoch [10/10], Iter [98/45] Loss: 0.8306\n",
      "Epoch [10/10], Iter [99/45] Loss: 1.0525\n",
      "Epoch [10/10], Iter [100/45] Loss: 0.9852\n",
      "Epoch [10/10], Iter [101/45] Loss: 0.9642\n",
      "Epoch [10/10], Iter [102/45] Loss: 1.0637\n",
      "Epoch [10/10], Iter [103/45] Loss: 0.9027\n",
      "Epoch [10/10], Iter [104/45] Loss: 1.3049\n",
      "Epoch [10/10], Iter [105/45] Loss: 0.9405\n",
      "Epoch [10/10], Iter [106/45] Loss: 1.0015\n",
      "Epoch [10/10], Iter [107/45] Loss: 0.9798\n",
      "Epoch [10/10], Iter [108/45] Loss: 0.4521\n",
      "Epoch [10/10], Iter [109/45] Loss: 0.9562\n",
      "Epoch [10/10], Iter [110/45] Loss: 0.7007\n",
      "Epoch [10/10], Iter [111/45] Loss: 1.1283\n",
      "Epoch [10/10], Iter [112/45] Loss: 0.9963\n",
      "Epoch [10/10], Iter [113/45] Loss: 0.6426\n",
      "Epoch [10/10], Iter [114/45] Loss: 0.9996\n",
      "Epoch [10/10], Iter [115/45] Loss: 0.9436\n",
      "Epoch [10/10], Iter [116/45] Loss: 0.7825\n",
      "Epoch [10/10], Iter [117/45] Loss: 1.1851\n",
      "Epoch [10/10], Iter [118/45] Loss: 0.8799\n",
      "Epoch [10/10], Iter [119/45] Loss: 0.8501\n",
      "Epoch [10/10], Iter [120/45] Loss: 0.8606\n",
      "Epoch [10/10], Iter [121/45] Loss: 0.8748\n",
      "Epoch [10/10], Iter [122/45] Loss: 0.9022\n",
      "Epoch [10/10], Iter [123/45] Loss: 0.9166\n",
      "Epoch [10/10], Iter [124/45] Loss: 1.2421\n",
      "Epoch [10/10], Iter [125/45] Loss: 1.0664\n",
      "Epoch [10/10], Iter [126/45] Loss: 1.1520\n",
      "Epoch [10/10], Iter [127/45] Loss: 0.5779\n",
      "Epoch [10/10], Iter [128/45] Loss: 1.3932\n",
      "Epoch [10/10], Iter [129/45] Loss: 0.8234\n",
      "Epoch [10/10], Iter [130/45] Loss: 1.0769\n",
      "Epoch [10/10], Iter [131/45] Loss: 1.3286\n",
      "Epoch [10/10], Iter [132/45] Loss: 1.3583\n",
      "Epoch [10/10], Iter [133/45] Loss: 0.6780\n",
      "Epoch [10/10], Iter [134/45] Loss: 0.9390\n",
      "Epoch [10/10], Iter [135/45] Loss: 0.4430\n",
      "Epoch [10/10], Iter [136/45] Loss: 0.6751\n",
      "Epoch [10/10], Iter [137/45] Loss: 1.0237\n",
      "Epoch [10/10], Iter [138/45] Loss: 1.2494\n",
      "Epoch [10/10], Iter [139/45] Loss: 1.3024\n",
      "Epoch [10/10], Iter [140/45] Loss: 1.0427\n",
      "Epoch [10/10], Iter [141/45] Loss: 1.2627\n",
      "Epoch [10/10], Iter [142/45] Loss: 1.0220\n",
      "Epoch [10/10], Iter [143/45] Loss: 1.0069\n",
      "Epoch [10/10], Iter [144/45] Loss: 1.0846\n",
      "Epoch [10/10], Iter [145/45] Loss: 0.9982\n",
      "Epoch [10/10], Iter [146/45] Loss: 0.7649\n",
      "Epoch [10/10], Iter [147/45] Loss: 0.8904\n",
      "Epoch [10/10], Iter [148/45] Loss: 1.0724\n",
      "Epoch [10/10], Iter [149/45] Loss: 0.8251\n",
      "Epoch [10/10], Iter [150/45] Loss: 0.7868\n",
      "Epoch [10/10], Iter [151/45] Loss: 0.7491\n",
      "Epoch [10/10], Iter [152/45] Loss: 0.8210\n",
      "Epoch [10/10], Iter [153/45] Loss: 1.0174\n",
      "Epoch [10/10], Iter [154/45] Loss: 0.9353\n",
      "Epoch [10/10], Iter [155/45] Loss: 0.9302\n",
      "Epoch [10/10], Iter [156/45] Loss: 0.5514\n",
      "Epoch [10/10], Iter [157/45] Loss: 1.0257\n",
      "Epoch [10/10], Iter [158/45] Loss: 1.0313\n",
      "Epoch [10/10], Iter [159/45] Loss: 0.8901\n",
      "Epoch [10/10], Iter [160/45] Loss: 0.8130\n",
      "Epoch [10/10], Iter [161/45] Loss: 0.8014\n",
      "Epoch [10/10], Iter [162/45] Loss: 0.5048\n",
      "Epoch [10/10], Iter [163/45] Loss: 0.9072\n",
      "Epoch [10/10], Iter [164/45] Loss: 1.0017\n",
      "Epoch [10/10], Iter [165/45] Loss: 1.0084\n",
      "Epoch [10/10], Iter [166/45] Loss: 1.0841\n",
      "Epoch [10/10], Iter [167/45] Loss: 0.9500\n",
      "Epoch [10/10], Iter [168/45] Loss: 1.0722\n",
      "Epoch [10/10], Iter [169/45] Loss: 1.2559\n",
      "Epoch [10/10], Iter [170/45] Loss: 1.0813\n",
      "Epoch [10/10], Iter [171/45] Loss: 1.0457\n",
      "Epoch [10/10], Iter [172/45] Loss: 0.8086\n",
      "Epoch [10/10], Iter [173/45] Loss: 0.7424\n",
      "Epoch [10/10], Iter [174/45] Loss: 1.0203\n",
      "Epoch [10/10], Iter [175/45] Loss: 0.9684\n",
      "Epoch [10/10], Iter [176/45] Loss: 1.4110\n",
      "Epoch [10/10], Iter [177/45] Loss: 1.2712\n",
      "Epoch [10/10], Iter [178/45] Loss: 0.4477\n",
      "Epoch [10/10], Iter [179/45] Loss: 1.2576\n",
      "Epoch [10/10], Iter [180/45] Loss: 0.7799\n",
      "10\n",
      "10\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOefKeKHgnHOujBcKzjnnyvz/HHBHvZKgIl4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8b81349b50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlFJhThOBUkqFOU0ESikV5v4fe37sYVXY0mYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8b80c0d750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "#matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "# coding: utf-8\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import time\n",
    "import os\n",
    "import torch.utils.data as data_utils\n",
    "import csv\n",
    "from os import listdir\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "\n",
    "def plot_loss_acc(list1,label1,list2,label2,title,filename):\n",
    "    \n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax1.plot(range(len(list1)), list1, 'C0', label=label1)\n",
    "    ax1.plot(range(len(list2)), list2, 'C1', label = label2)\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    # Make the y-axis label, ticks and tick labels match the line color.\n",
    "    ax1.set_ylabel(title)\n",
    "    ax1.tick_params('y')  \n",
    "    ax1.legend(loc = 1)\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    plt.savefig(filename)\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "torch.cuda.is_available()\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "#Ultra Parameters\n",
    "num_images = 5000\n",
    "shuffle = True\n",
    "random_seed = 16\n",
    "valid_size = 0.1\n",
    "\n",
    "#Hyper Parameters\n",
    "num_epochs = 10\n",
    "batch_size = 25\n",
    "#learning_rate = 0.001\n",
    "'''\n",
    "This function takes in an image's path and returns \n",
    "a numpy array that contains all its pixel values.\n",
    "The dimension of the output array is (channels, width, height).\n",
    "And in our case, channels = 1.\n",
    "'''\n",
    "def get_image(image_path):\n",
    "    image = Image.open(image_path, 'r')\n",
    "    maxsize = (224, 224)\n",
    "    image.thumbnail(maxsize, Image.ANTIALIAS)\n",
    "    width, height = 224, 224\n",
    "#    if image.mode != 'L':\n",
    "#        image = image.convert('L')\n",
    "    pixel_values = list(image.getdata())\n",
    "    #pixel_values = np.array(pixel_values).reshape((width, height))\n",
    "    #pixel_values = preprocessing.scale(pixel_values)\n",
    "    pixel_values = np.array(pixel_values).reshape((1, width, height))\n",
    "    pixel_values.astype(dtype=np.float16)\n",
    "    #pixel_values = np.array(pixel_values)\n",
    "    return pixel_values\n",
    "\n",
    "'''\n",
    "Just loop through all the images in the image folder\n",
    "and call convert each of them to an array.\n",
    "The dimension of output is (num of images, (channels, width, height))\n",
    "'''\n",
    "\n",
    "def loadImages(directory):\n",
    "    global num_images\n",
    "    imageList = listdir(directory)\n",
    "    loadedImages = []\n",
    "    index = 0\n",
    "    for image in imageList:\n",
    "        if index == num_images: break\n",
    "        pixel_values = get_image(directory + image)\n",
    "        loadedImages.append(pixel_values)\n",
    "        if index % 100 == 0:\n",
    "            print \"Loading Image: \", index        \n",
    "        index += 1\n",
    "\t#print index\n",
    "    return loadedImages\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Function name explains itself\n",
    "'''\n",
    "def constructLabelByString(labelStr):\n",
    "    label_vector = [0] * 14\n",
    "    if labelStr == 'No Finding':\n",
    "        return label_vector\n",
    "\n",
    "    symptom_vector = ['Atelectasis', 'Cardiomegaly', 'Effusion',     'Infiltration', 'Mass', 'Nodule', 'Pneumonia', 'Pneumothorax',     'Consolidation', 'Edema', 'Emphysema', 'Fibrosis',     'Pleural_Thickening', 'Hernia']\n",
    "    \n",
    "    symptoms = labelStr.split('|')\n",
    "    for s in symptoms:\n",
    "        label_vector[symptom_vector.index(s)] = 1\n",
    "    return label_vector\n",
    "   \n",
    "'''\n",
    "Generate labels\n",
    "'''\n",
    "def parseDataEntry(dataEntryPath):   \n",
    "    #Each element in the map is a tuple (image filename, corresponding label)\n",
    "    labels = []\n",
    "    index = 0\n",
    "    with open('/datasets/ChestXray-NIHCC/Data_Entry_2017.csv', 'rb') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            if index == 0: \n",
    "                index += 1\n",
    "                continue\n",
    "            label_vector = constructLabelByString(row[1])\n",
    "            labels.append(label_vector)\n",
    "    return labels\n",
    "            \n",
    "#Image Path\n",
    "#image_dir = '/datasets/tmp/tum/'\n",
    "#these are tensor floats already\n",
    "#image_dir = '/datasets/tmp/tum_torch/001/'\n",
    "image_dir = '/datasets/tmp/tum/'\n",
    "data_entry_path = '/datasets/ChestXray-NIHCC/Data_Entry_2017.csv'\n",
    "\n",
    "\n",
    "#Get labels\n",
    "loadedLabels = np.array(parseDataEntry(data_entry_path)[:num_images])\n",
    "print \"done loading labels\"\n",
    "\n",
    "#old method\n",
    "loadedImages = np.array(loadImages(image_dir))\n",
    "print \"done loading images\"\n",
    "\n",
    "#Load Images new method\n",
    "#loadedImages = torch.stack(loadImages(image_dir,num_images))\n",
    "#print \"done loading images\"\n",
    "\n",
    "\n",
    "#new loading \n",
    "#features = loadedImages.float()\n",
    "#old method\n",
    "features = torch.from_numpy(loadedImages).float()\n",
    "\n",
    "targets = torch.from_numpy(loadedLabels).float()\n",
    "train_dataset = data_utils.TensorDataset(features, targets)\n",
    "valid_dataset = data_utils.TensorDataset(features, targets)\n",
    "\n",
    "#Using SubsetRandomSampler for 1-fold cross validation\n",
    "\n",
    "num_train = len(train_dataset)\n",
    "indices = list(range(num_train))\n",
    "split = int(np.floor(valid_size*num_train))\n",
    "if shuffle == True:\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "train_sampler = data_utils.sampler.SubsetRandomSampler(train_idx)\n",
    "test_sampler = data_utils.sampler.SubsetRandomSampler(indices[split+200:split+699])\n",
    "valid_sampler = data_utils.sampler.SubsetRandomSampler(valid_idx)\n",
    "\n",
    "train_loader = data_utils.DataLoader(train_dataset, batch_size = batch_size,                                      sampler = train_sampler, pin_memory = True)\n",
    "test_loader = data_utils.DataLoader(train_dataset, batch_size = 1, sampler = test_sampler, pin_memory = True)\n",
    "valid_loader = data_utils.DataLoader(valid_dataset, batch_size = 1,                                      sampler = valid_sampler, pin_memory = True)\n",
    "\n",
    "\n",
    "# create custom Alexnet\n",
    "class CustomAlexnet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CustomAlexnet, self).__init__()\n",
    "        alex_model = models.alexnet(pretrained=True)        \n",
    "        self.features = alex_model.features\n",
    "        self.classifier = nn.Sequential(\n",
    "            *[list(alex_model.classifier.children())[i] for i in [0,1,2,3,4,5]]\n",
    "        )\n",
    "        \n",
    "        mod = list(self.classifier.children())\n",
    "        mod.append(nn.Linear(4096, num_classes))\n",
    "        new_classifier = torch.nn.Sequential(*mod)\n",
    "        self.classifier = new_classifier \n",
    "        \n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        for layer_idx, param in enumerate(self.classifier.parameters()):\n",
    "            if layer_idx > 4:\n",
    "                param.requires_grad = True         \n",
    "            else:\n",
    "                pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.cat((x,x,x),1)\n",
    "        x = self.features(x)\n",
    "#        x = torch.squeeze(x)\n",
    "#        print x.size()\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        x = F.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "#hyperparams\n",
    "num_epochs = 10\n",
    "batch_size = 100\n",
    "learning_rate = 0.05\n",
    "    \n",
    "# load custom model\n",
    "amine_alexnet = CustomAlexnet(num_classes=14)\n",
    "#parameters = list(amine_alexnet.linear.parameters())\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, amine_alexnet.parameters()), lr=learning_rate)\n",
    "#optimizer = optim.Adam(params=, lr=learning_rate)\n",
    "#optimizer = torch.optim.SGD(params=amine_alextnet.parameters(), lr=0.01)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "amine_alexnet.cuda()        \n",
    "\n",
    "# In[18]:\n",
    "\n",
    "def validation(loader, criterion):\n",
    "        lossList = []\n",
    "        # Test the Model\n",
    "        amine_alexnet.eval()\n",
    "        correct_naive = 0\n",
    "        correct_tolerance = 0\n",
    "        correct_absolute = 0\n",
    "        total = 0\n",
    "        for images, labels in valid_loader:\n",
    "            images = Variable(images).cuda()\n",
    "\t    lll = Variable(labels).cuda()\n",
    "            outputs = amine_alexnet(images)\n",
    "            loss = criterion(outputs, lll)\n",
    "            lossList.append(loss.data[0])\n",
    "            prediction = outputs.data.cpu().numpy()\n",
    "            prediction[prediction >= 0.6] = 1\n",
    "            prediction[prediction < 0.6] = 0\n",
    "            total += labels.size(0)\n",
    "            labels = labels.numpy()\n",
    "            diff = np.count_nonzero(prediction != labels)\n",
    "            correct_rate = 1.0 - (float(diff) / 14.0)\n",
    "\treturn (sum(lossList) / len(lossList))\n",
    "\n",
    "trainingLossList = []\n",
    "validationLossList = []\n",
    "trainingAccList = []\n",
    "validationAccList = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epochLoss = 0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = Variable(images).cuda()\n",
    "        labels = Variable(labels).cuda()\n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()\n",
    "        outputs = amine_alexnet(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epochLoss += loss.data[0]\n",
    "        print ('Epoch [%d/%d], Iter [%d/%d] Loss: %.4f' \n",
    "        %(epoch+1, num_epochs, i+1, np.floor(len(train_dataset)*(1-valid_size))//batch_size, loss.data[0]))\n",
    "    trainingLossList.append(epochLoss / (len(train_dataset) / batch_size)/batch_size)\n",
    "    validationLossList.append(validation(valid_loader, criterion))\n",
    "    print len(validationLossList)\n",
    "    print len(trainingLossList)\n",
    "    \n",
    "    amine_alexnet.eval()  # Change model to 'eval' mode (BN uses moving mean/var).\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in valid_loader:\n",
    "        images = Variable(images).cuda()\n",
    "        outputs = amine_alexnet(images)\n",
    "        prediction = outputs.data.cpu().numpy()\n",
    "        prediction[prediction >= 0.6] = 1\n",
    "        prediction[prediction < 0.6] = 0\n",
    "        total += labels.size(0)\n",
    "        labels = labels.numpy()\n",
    "        #print (\"labels = \", labels)\n",
    "        #print (\"prediction = \", prediction)\n",
    "        #print np.array_equal(prediction, labels)\n",
    "        #raw_input()\n",
    "        diff = np.count_nonzero(prediction != labels)\n",
    "        correct_rate = 1.0 - (float(diff) / 14.0)\n",
    "        correct+= correct_rate\n",
    "    validationAccList.append(100*correct/total)\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = Variable(images).cuda()\n",
    "        outputs = amine_alexnet(images)\n",
    "        prediction = outputs.data.cpu().numpy()\n",
    "        prediction[prediction >= 0.6] = 1\n",
    "        prediction[prediction < 0.6] = 0\n",
    "        total += labels.size(0)\n",
    "        labels = labels.numpy()\n",
    "        #print (\"labels = \", labels)\n",
    "        #print (\"prediction = \", prediction)\n",
    "        #print np.array_equal(prediction, labels)\n",
    "        #raw_input()\n",
    "        diff = np.count_nonzero(prediction != labels)\n",
    "        correct_rate = 1.0 - (float(diff) / 14.0)\n",
    "        correct+= correct_rate\n",
    "    trainingAccList.append(100*correct/total)\n",
    "    \n",
    "    amine_alexnet.train()\n",
    "\n",
    "plot_loss_acc(trainingLossList, 'Training Loss', validationLossList, 'Validation Loss', 'BCELoss on Training\\Validation Set vs Epochs', 'alex_lossplot.png')\n",
    "# In[19]:\n",
    "\n",
    "\n",
    "# Test the Model\n",
    "\n",
    "plt.plot(range(1,num_epochs+1),validationAccList,label = 'Validation Accuracy')\n",
    "plt.plot(range(1,num_epochs+1),trainingAccList,label = 'Training Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Train/Validation Accuracy vs Epochs')\n",
    "plt.show()\n",
    "\n",
    "#print('Test Accuracy of the model: %d %% (Total: %d; Correct: %d)' % (100 * correct / total, total, correct))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
